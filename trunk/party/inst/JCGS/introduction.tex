
\begin{center}
\section{INTRODUCTION}
\end{center}

Statistical models that regress the distribution of a response variable
on the status of multiple covariates are tools for handling two major
problems in applied research:
prediction and explanation. The function space represented by
regression models
focusing on the prediction problem may be arbitrarily complex; indeed, 
`black
box' systems like support vector machines or ensemble methods are excellent
predictors. In contrast, regression models appropriate for gaining insight
into the
mechanism of the data generating process are required to offer
a human readable representation. Generalized linear models or
the Cox model are representatives of regression models where 
parameter estimates of the coefficients and their distribution
are used to judge the relevance of single covariates.

With their seminal work on automated interaction detection (AID),
\cite{MorganSonquist1963} introduced another class of simple  
regression models for prediction and explanation 
nowadays known as `recursive partitioning' or `trees'.
Many variants and extensions have been published in the last $40$
years, the majority of which are special cases of a simple two-stage 
algorithm: 
first partition the observations by univariate splits in a recursive way and 
second fit a constant model in each cell of the resulting partition. 
The most popular implementations of such algorithms are `CART' 
\citep{classifica:1984} and `C4.5' \citep{Quinlan1993}. Not unlike AID, 
both perform an exhaustive search over all possible splits maximizing an
information measure of node impurity selecting the 
covariate showing the best split.
This approach has two fundamental problems: overfitting and a selection 
bias towards covariates with many possible splits. 
With respect to the 
overfitting problem \cite{Mingers1987} notes that the algorithm
\begin{quote}
[\ldots] has no concept of statistical significance, and so cannot
distinguish between a significant and an insignificant improvement in the
information measure.
\end{quote}
Within the exhaustive search framework, pruning procedures, 
mostly based on some form of cross-validation, 
are necessary to restrict the number of cells in the resulting partitions in
order to avoid overfitting problems. 
While pruning is successful in selecting the right-sized tree, the
interpretation of the trees is affected by the biased variable selection.  
This bias is induced by maximizing a splitting criterion over all
possible splits simultaneously and was identified as a problem by many
researchers \citep[e.g.,][p.~42]{Kass1980,regression:1988,classifica:1984}. 
The nature of the variable selection problem under different circumstances
has been studied intensively \citep{WhiteLiu1994,JensenCohen2000,Shih2004}
and \cite{classifica:2001} argue that exhaustive search methods are
biased towards variables with many missing values as well. 
With this article we enter at the point where \cite{WhiteLiu1994} demand for 
\begin{quote}
[\ldots] a \textit{statistical} approach [to recursive partitioning] which takes
into account the \textit{distributional} properties of the measures.
\end{quote} 
We present a unified framework embedding recursive binary partitioning with
piecewise constant fits into 
the well-defined theory of permutation tests developed by \cite{StrasserWeber1999}. 
The conditional distribution of statistics measuring the association between
responses and covariates is the basis for an unbiased selection among
covariates measured at different scales. 
Moreover, multiple test procedures are applied to determine whether no significant 
association between any of the covariates and the response can be stated and 
the recursion needs to stop. We show that such 
statistically motivated stopping criteria implemented via hypothesis tests lead
to regression models whose predictive performance is equivalent to the
performance of optimally pruned trees,
%%obtained by well established    
%%exhaustive search methods, 
therefore offering an intuitive and computationally
efficient solution to the overfitting problem.

The development of the framework presented here was inspired by various 
attempts 
to solve both the overfitting and variable selection
problem published in the last $25$ years \citep[a far more
detailed overview is given by][]{Murthy1998}. 
The $\chi^2$ automated interaction detection 
algorithm \citep[`CHAID', ][]{Kass1980} is the first approach based on 
statistical significance tests for contingency tables. 
The basic idea of this algorithm is the separation of the variable selection and
splitting procedure. The significance of the association between a nominal
response and one of the covariates is investigated by a $\chi^2$ test 
and the covariate with highest association is selected for splitting.
Consequently, this algorithm has a concept of statistical significance and a
criterion to stop the algorithm can easily be implemented based on formal   
hypothesis tests. 
%%The software package FIRM \citep{Hawkins1995} 
%%essentially implements the CHAID algorithm.
%%For a binary response
%%variable and continuous covariates, \cite{Rounds1980} proposes an algorithm
%%utilizing the distribution of the Kolmogorov-Smirnov statistic. 

A series of papers aiming at unbiased recursive partitioning for nominal and continuous 
responses starts with `FACT' \citep{LohVanichsetakul1988}, where covariates are
selected within an analysis of variance (ANOVA) 
framework treating a nominal response
as the independent variable. Basically, the covariate with largest $F$-ratio is
selected for splitting. Nominal covariates are coerced to ordered variables
via the canonical variate of the corresponding matrix of dummy codings. This
induces a biased variable selection when nominal covariates are present and
therefore `QUEST' \citep{LohShih1997} addresses this problem by selecting
covariates on a $P$-value scale. 
For continuous variables, $P$-values are derived from the
corresponding ANOVA $F$-statistics and for nominal covariates a $\chi^2$
test is applied. This approach reduces the variable selection bias
substantially. 
Further methodological developments within this framework
include the incorporation of a linear
discriminant analysis model within each node of a tree \citep{KimLoh2003}
and multiway splits \citep[`CRUISE',][]{classifica:2001}.
For continuous responses, `GUIDE' \citep{Loh2002} seeks to implement
unbiasedness by a different approach. Here, the association between the sign
of model residuals and each covariate is measured by a $P$-value derived
from a $\chi^2$ test. Continuous covariates are categorized to four
levels prior to variable selection; however, models are 
fitted to untransformed covariates in the nodes.
These approaches are already very successful in reducing the variable 
selection bias and typically perform very well in the partitioning tasks
they were designed for. Building on these ideas, we introduce a new unifying
conceptual framework for unbiased recursive partitioning based on 
conditional hypothesis testing that, in addition to models for continuous and
categorical data, includes procedures applicable to
censored, ordinal or multivariate responses.

%%Although these variants do not suffer a severe variable selection bias, they are
%%not generally applicable, require
%%to categorize continuous covariates prior to modeling (CHAID, GUIDE), 
%%or treat covariates as dependent variables 
%%when the association to the response is measured by 
%%ANOVA (FACT, QUEST) or Kolmogorov-Smirnov statistics. 
%%Moreover, some assumptions to the 
%%data generating  process need to be made in order to fulfill the 
%%requirements of parametric test procedures.  

%%\cite{Su2004} re-formulate recursive partitioning within a maximum
%%likelihood framework. Well established measures like the Akaike or Bayesian
%%information criterion can than be utilized to prune large initial trees.
%%However, the variable selection bias remains a problem since the likelihood
%%is maximized by means of an exhaustive search procedure.



Previous attempts to implement permutation (or randomization) tests in
recursive partitioning algorithms aimed at solving the variable selection
and overfitting problem \citep{JensenCohen2000}, however focusing on special
situations only. 
Resampling procedures have been employed for assessing split
statistics for censored responses by \cite{survival-t:1993}.
\cite{FrankWitten1998} utilize the conditional Monte-Carlo approach for the
approximation of the distribution of Fisher's exact test for nominal
responses and the conditional probability of an observed contingency table
is used by \cite{Martin1997}. 
The asymptotic distribution of a $2 \times 2$ table obtained by maximizing
the $\chi^2$ statistic over possible splits in a continuous covariate is  
derived by \cite{maximally-:1982}. Maximally selected rank statistics 
\citep{maximally-:1992} can be
applied to continuous and censored responses as well and are  
applied to correct the bias of exhaustive search recursive partitioning by
\cite{Lausen2004}.
An approximation to the distribution of the Gini criterion
is given by \cite{DobraGehrke2001}. However, lacking solutions for more
general situations,  these auspicious approaches are hardly ever applied 
and the majority of tree-structured regression models reported and
interpreted in applied research papers is biased. 
The main reason is that computationally efficient solutions are
available for special cases only. 

The framework presented in
Section~\ref{framework} is efficiently applicable to regression
problems where both response and covariates can be measured at arbitrary
scales, including nominal, ordinal, discrete and
continuous as well as censored and multivariate variables.
The treatment of special situations is explained in Section~\ref{examples}
and applications including glaucoma classification, node
positive breast cancer survival and a questionnaire on mammography experience
illustrate the methodology in Section~\ref{illustrations}. 
Finally, we show by benchmarking experiments that  
recursive partitioning based on statistical criteria 
as introduced in this paper lead to regression 
models whose predictive performance is as good as the performance of optimally 
pruned trees.
