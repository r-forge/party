\documentclass{Z}

%% packages
\usepackage{amsmath,fancyheadings}

%% Sweave: need no \usepackage{Sweave}
\SweaveOpts{engine=R,eps=FALSE}

%% commands
\newcommand{\ui}{\underline{i}}
\newcommand{\oi}{\overline{\imath}}
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}

%% author/title
\author{Achim Zeileis\\Wirtschaftsuniversit\"at Wien \And
        Torsten Hothorn\\Ludwig-Maximilians-Universit\"at\\M\"unchen \And
	Kurt Hornik\\Wirtschaftsuniversit\"at Wien}
\Plainauthor{Achim Zeileis, Torsten Hothorn, Kurt Hornik}
\title{Model-based Recursive Partitioning}
\Keywords{change points, maximum likelihood, parameter instability, recursive partitioning}

%% abstract
\Abstract{
  Recursive partitioning is embedded into the general and well-established
  class of parametric models that can be fitted using M-type estimators (including
  maximum likelihood).
  An algorithm for model-based recursive partitioning is suggested  
  for which the basic steps are: (1)~fit a parametric model to a data
  set, (2)~test for parameter instability over a set of partitioning variables,
  (3)~if there is some overall parameter instability, split the model with
  respect to the variable associated with the highest instability, (4)~repeat
  the procedure in each of the daughter nodes. The algorithm yields a
  partitioned (or segmented) parametric model that can be effectively 
  visualized and that subject-matter scientists are used to analyzing and
  interpreting.
}

\begin{document}

\pagestyle{fancy}
\thispagestyle{fancyplain}
\headrulewidth0pt
\lhead[]{}
\chead[]{}
\rhead[]{}
\lfoot[This is a preprint of an article accepted for publication in \textit{Journal of Computational and Graphical Statistics}.
Copyright {\copyright} 2007 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America]%
{This is a preprint of an article accepted for publication in \textit{Journal of Computational and Graphical Statistics}.
Copyright {\copyright} 2007 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America}
\cfoot[]{}
\rfoot[]{}


<<preliminaries,echo=FALSE,results=hide>>=
library("party")
library("multcomp")
options(SweaveHooks=list(twofig=function() {par(mfrow=c(1,2))},
                         onefig=function() {par(mfrow=c(1,1))}))
Pval <- function(p, digits = 3)
  ifelse(p <= 10^(-digits),
    paste("p < 0.", paste(rep("0", digits-1), collapse = "") , "1", sep = ""),
    paste("p =", as.character(round(p, digits = digits))))
get_version <- function(pkg) packageDescription(pkg)["Version"]
@

\section{Introduction}
\label{sec:intro}

Since the appearance of the first tree-structured regression analysis 
\citep[Automated Interaction Detection,][]{MorganSonquist1963},
virtually every publication in this field highlights two features of trees: 
(1)~interpretability---enhanced by visualizations of the fitted decision trees---and
(2)~predictive power in non-linear regression relationships.
The latter is of diminishing importance because modern approaches to
predictive modeling such as boosting \citep[e.g., simple $L_2$ boosting by][]{BuehlmannYu2003},
random forests \citep{Breiman2001} or support vector machines \citep{Vapnik1996}
are often found to be superior to trees in purely predictive settings
\citep[e.g.,][]{Meyeretal2003}.
However, a simple graphical representation of a complex regression problem is
still very valuable, probably increasingly so.

In the last decade, the incorporation of (simple) parametric models into
trees has been receiving increased interest. Research in this direction was
mainly motivated by the fact that constant fits in each node
tend to produce large and thus hard to interpret trees
\citep[see e.g.,][]{ChanLoh2004}. Several algorithms have been suggested 
both in the statistical and machine learning communities that attach 
parametric models to terminal nodes or employ linear combinations to obtain
splits in inner nodes. In machine learning, such approaches are known as 
\textit{hybrid}, \textit{model} or \textit{functional trees} \citep{Gama2004} with 
M5 \citep{Quinlan1993} being the most prominent representative.
The key developments in statistics are due to Wei-Yin Loh and his
coworkers. GUIDE \citep{Loh2002}, CRUISE \citep{KimLoh2001} and
LOTUS \citep{ChanLoh2004} attach parametric models to terminal nodes,
and \cite{Choietal2005} suggest an extension to count data.
Some of these algorithms \citep[in particular CRUISE or QUEST,][]{LohShih1997}
additionally allow one to employ parametric
models to obtain splits in inner nodes. Furthermore, maximum likelihood
trees \citep{Suetal2004} embed regression trees with a constant fit
in each terminal node into maximum likelihood estimation.

Building on these ideas, we carry the integration of parametric models
into trees one step further and provide
a rigorous theoretical foundation by introducing a new unified framework
that embeds recursive partitioning into statistical model estimation
and variable selection. Within this framework, a segmented
parametric model is fitted by computing a tree in which every leaf is associated
with a fitted model such as, e.g., a maximum likelihood model or a
linear regression. The model's objective function is used for estimating
the parameters and the split points; the corresponding model scores
are tested for parameter instability in each node to assess
which variable should be used for partitioning.
The benefits of employing this approach are: The
objective function used for parameter estimation is also used for
partitioning (testing \textit{and} split point estimation).
The recursive partitioning allows for modeling of
non-linear relationships and automated detection of interactions among
the explanatory variables. The statistical formulation of the algorithm
ensures the validity of interpretations drawn from the resulting model.
Moreover, the use of well-known parametric models provides
subject-matter scientists with a segmented model that they are used to
analyzing and interpreting.


\headrulewidth0pt
\lhead[\thepage]{}
\chead[\textit{Model-based Recursive Partitioning}]{\textit{Achim Zeileis, Torsten Hothorn, Kurt Hornik}}
\rhead[]{\thepage}
\lfoot[{\small Copyright {\copyright} 2007 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America}]{{\small Copyright {\copyright} 2007 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America}}
\cfoot[]{}
\rfoot[]{}

The remainder of the paper is organized as follows: Section~\ref{sec:segmented}
establishes the class of models the framework is based on before Section~\ref{sec:algorithm} describes
the suggested model-based recursive partitioning algorithm in detail. Section~\ref{sec:illustration}
provides a set of illustrations and applications along with benchmark
comparisons with other tree-based algorithms. Section~\ref{sec:discussion}
provides a short discussion of some further details of the algorithm before
Section~\ref{sec:summary} concludes the paper with a summary.

\section{Segmented models}
\label{sec:segmented}

Consider a parametric model $\mathcal{M}(Y, \theta)$
with (possibly vector-valued) observations $Y \in \mathcal{Y}$ and a
$k$-dimensional vector of parameters $\theta \in \Theta$.
Given $n$ observations $Y_i$ ($i = 1, \dots, n$) the model can be fitted
by minimizing some objective function $\Psi(Y, \theta)$ yielding the
parameter estimate $\hat \theta$
\begin{equation} \label{eq:Psi}
\hat \theta \quad = \quad \argmin_{\theta \in \Theta}
\sum_{i = 1}^n \Psi(Y_i, \theta).
\end{equation}
Estimators of this type include various well-known estimation techniques, the most
popular being ordinary least squares (OLS) or maximum likelihood (ML) among other
M-type estimators. In the case of OLS, $\Psi$ is typically the error sum of squares
and, in the case of ML, it is the negative log-likelihood. In the latter case,
it could be the full likelihood of the variable $Y$ or the conditional likelihood
if $Y$ can be split into dependent and explanatory variables $Y = (y, x)^\top$.

%% \dotfill

\textbf{Example:} (Multivariate) normal distribution. The observations
$Y$ are normally distributed with mean $\mu$ and covariance matrix $\Sigma$:
$Y \sim \mathcal{N}(\mu, \Sigma)$ with
the combined parameter vector $\theta = (\mu, \Sigma)$.
%% \begin{equation} \label{eq:normal}
%% Y \quad \sim \quad \mathcal{N}(\mu, \Sigma)
%% \end{equation}

\textbf{Example:} Generalized linear model (GLM). The observations
can be split into a dependent variable $y$ and covariates or regressors
$x$, i.e., $Y = (y, x)^\top$. The model equation is
$g(\E(y)) = x^\top \theta$
%% \begin{equation} \label{eq:GLM}
%% g(\E(y)) \quad = \quad x^\top \theta,
%% \end{equation}
where $y$ has a pre-specified exponential family distribution,
$g(\cdot)$ is a known link function and $\theta$ are the regression coefficients.

%% \dotfill

In many situations, it is unreasonable to assume that a single global model
$\mathcal{M}(Y, \theta)$ fits all $n$ observations well. But it might be possible
to partition the observations with respect to some covariates
such that a well-fitting model can be found locally in each cell of the partition. In such a
situation, we can use a recursive partitioning approach based
on $\ell$ partitioning variables $Z_j \in \mathcal{Z}_j \; (j = 1, \dots, \ell)$
to adaptively find a good approximation of this partition.

More formally, we assume that a partition $\{\mathcal{B}_b\}_{b = 1, \dots, B}$  of the space
$\mathcal{Z} = \mathcal{Z}_1 \times \dots \times \mathcal{Z}_\ell$
exists with $B$ cells (or segments) such that in each cell $\mathcal{B}_b$
a model $\mathcal{M}(Y, \theta_b)$ with a cell-specific 
parameter $\theta_b$ holds.
We denote this segmented model by $\mathcal{M}_\mathcal{B}(Y, \{\theta_b\})$
where $\{\theta_b\}_{b = 1, \dots, B}$ is the full combined parameter.

Special cases of such segmented models are classification and regression trees
where many partitioning variables $Z_j$ but only very simple models $\mathcal{M}$
are used, and structural break models that find partitions with respect to time.

%% \dotfill

\textbf{Example:}
For regression trees a simple model $\mathcal{M}$ is chosen: the parameter
$\theta$ describes the mean of the univariate observations $Y_i$ and is
estimated by OLS (or equivalently ML in a normal model with the variance
treated as a nuisance parameter). The variables $Z_j$
are the regressors considered for partitioning.

\textbf{Example:}
In change point or structural change analysis, typically a linear regression
model with $Y_i = (y_i, x_i)^\top$ and regression coefficients $\theta$
is segmented with respect to only a single variable $Z_1$ (i.e., $\ell = 1$)
which is usually time \citep{BaiPerron2003,ZeileisKleiberKraemer2003}.

%% \dotfill

Given the correct partition $\{\mathcal{B}_b\}$ the estimation of the parameters $\{\theta_b\}$
that minimize the global objective function can easily be achieved by computing the locally
optimal parameter estimates $\hat \theta_b$ in each segment $\mathcal{B}_b$. However,
if $\{\mathcal{B}_b\}$ is unknown, minimization of the global objective function
\begin{equation} \label{eq:PsiB}
\sum_{b = 1}^B \sum_{i \in I_b} \Psi(Y_i, \theta_b) \quad \rightarrow \quad \min,
\end{equation}
over all conceivable partitions $\{\mathcal{B}_b\}$ (with corresponding indexes
$I_b$, $b = 1, \dots, B$) is more complicated, even if the number of segments $B$ is fixed:
If there is more than one partitioning variable ($\ell > 1$), the number of potential 
partitions quickly becomes too large for an exhaustive search. If, in addition,
the number of segments $B$ is unknown, the challenges become even more severe---at least,
if trivial partitions, such as the partition, where each observation is its own segment,
are excluded, e.g.,  by requiring some minimum segment size. Furthermore, in this case,
some means should be taken to avoid overfitting by increasing $B$.

In short, determining the optimal partition (with respect to $\Psi$) is difficult,
even for fixed $B$. However, if there is only partitioning variable ($\ell = 1$),
the optimal split(s) can be found easily: both the statistics and econometrics
literature on change point and structural change analysis discuss various algorithms
for segmenting models over a single variable, typically time.
To exploit this methodology for finding a partition close to the optimal one in
$\ell > 1$ dimensions, we suggest a greedy forward search where the objective function
$\Psi$ can at least be optimized locally in each step. A detailed description of
this algorithm is given in the next section.

\section{The recursive partitioning algorithm}
\label{sec:algorithm}

The basic idea is that each node is associated with a single model.
To assess whether splitting of the node is necessary, a fluctuation test
for parameter instability is performed. If there is significant instability
with respect to any of the partitioning variables $Z_j$, split the node into 
$B$ locally optimal segments and repeat the procedure. If no more significant
instabilities can be found, the recursion stops and returns a tree where each
terminal node (or leaf) is associated with a model of type $\mathcal{M}(Y, \theta)$.
More precisely, the steps of the algorithm are

\begin{enumerate}
\item Fit the model once to all observations in the current node by estimating
  $\hat \theta$ via minimization of the objective function $\Psi$.
\item Assess whether the parameter estimates are stable with respect to
  every ordering $Z_1, \dots, Z_\ell$. If there is some overall instability,
  select the variable $Z_j$ associated with the highest parameter instability, otherwise
  stop.
\item Compute the split point(s) that locally optimize $\Psi$,
  either for a fixed or adaptively chosen number of splits.
\item Split the node into daughter nodes and repeat the procedure.
\end{enumerate}

The details for steps 1--3 are specified in the following. To keep
the notation simple, the dependence on the current segment is suppressed and
the symbols established for the global model are used, i.e., $n$ for the number
of observations in the current node, $\hat \theta$ for the associated parameter
estimate and $B$ for the number of daughter nodes chosen.


\subsection{Parameter estimation}

This step of the algorithm is common practice:
Under mild regularity conditions \citep[see e.g.,][]{White1994},
it can be shown that the estimate $\hat \theta$ defined by Equation~\ref{eq:Psi}
can also be computed by solving the first order conditions
\begin{equation} \label{eq:estfun}
\sum_{i = 1}^n \psi(Y_i, \hat \theta) \quad = \quad 0,
\end{equation}
where 
\begin{equation} \label{eq:psi}
\psi(Y, \theta) \quad = \quad \frac{\partial \Psi(Y, \theta)}{\partial \theta}
\end{equation}
is the score function or estimating function corresponding to $\Psi(Y, \theta)$.
Analytical closed form solutions for $\hat \theta$ are available only in certain special cases,
but for many models of interest well-established fitting algorithms for computing
$\hat \theta$ are available (e.g., OLS estimation via QR decomposition for linear regression
or ML via iterative weighted least squares for GLMs).
The score function evaluated at the estimated parameters
$\hat \psi_i = \psi(Y_i, \hat \theta)$ is then inspected for systematic deviations
from its mean $0$ in the next section.


\subsection{Testing for parameter instability}

The task in this step of the algorithm is to find out whether the parameters
of the fitted model are stable over each particular ordering implied by
the partitioning variables $Z_j$ or whether splitting the sample with respect
to one of the $Z_j$ might capture instabilities in the parameters and thus improve the fit.
To assess the parameter instability, a natural idea is to check whether the
scores $\hat \psi_i$ fluctuate randomly around their mean $0$ or exhibit
systematic deviations from $0$ over $Z_j$. These deviations can be captured
by the empirical fluctuation process

\begin{equation} \label{eq:Wj}
W_j (t) \quad = \quad {\hat J}^{-1/2} n^{-1/2}
\sum_{i = 1}^{\lfloor nt \rfloor} \hat \psi_{\sigma(Z_{ij})}
\qquad (0 \le t \le 1)
\end{equation}

where $\sigma(Z_{ij})$ is the ordering permutation which gives the antirank of the
observation $Z_{ij}$ in the vector $Z_j = (Z_{1j}, \dots, Z_{nj})^\top$.
Thus, $W_j(t)$ is simply the partial sum process of the scores ordered by
the variable $Z_j$, scaled by the number of observations $n$ and a 
suitable estimate
$\hat J$ of the covariance matrix $\COV(\psi(Y, \hat \theta))$, e.g.,
$\hat J = n^{-1} \sum_{i = 1}^n \psi(Y_i, \hat \theta) \psi(Y_i, \hat \theta)^\top$,
but other robust estimators such as HC (heteroskedasticity consistent) and HAC
(heteroskedasticity and autocorrelation consistent) estimators are also applicable.
%% \citep[see e.g.,][]{Z:Andrews:1991,Z:Lumley+Heagerty:1999,Z:Cribari-Neto:2004}.
%% If the estimator is order-dependent it would have to be re-computed for each
%% ordering.
This empirical fluctuation process is governed by a functional central limit theorem
\citep{ZeileisHornik2007} under the null hypothesis of parameter stability: it converges to a Brownian
bridge $W^0$. A test statistic can be derived
by applying a scalar functional $\lambda(\cdot)$ capturing the fluctuation in the
empirical process to the fluctuation process $\lambda(W_j(\cdot))$ and the corresponding
limiting distribution is just the same functional (or its asymptotic counterpart)
applied to the limiting process $\lambda(W^0(\cdot))$.


This very general framework for testing parameter stability is called
generalized M-fluctuation test and has been established by
\cite{ZeileisHornik2007}.  It has been shown to encompass a large number
of structural change tests suggested both in the econometrics and
statistics literature, including OLS-based CUSUM and MOSUM tests
\citep{PlobergerKraemer1992,ChuHornikKuan1995}, score-based tests
\citep{Nyblom1989,HjortKoning2002} and statistics based on Lagrange multiplier
statistics \citep{Andrews1993,AndrewsPloberger1994}---an overview is
given in \cite{Zeileis2005}. In principle, any of the tests from this
framework could be used in the recursive partitioning algorithm, but two
different test statistics seem to be particularly attractive for
assessing numerical and categorical partitioning variables $Z_j$
respectively.

\textbf{Assessing numerical variables:}
To capture the instabilities over a numerical variable $Z_j$, the following functional
is most intuitive:
\begin{equation} \label{eq:supLM}
\lambda_{\sup LM} (W_j) \quad = \quad
\max_{i = \ui, \dots, \oi}  \, \left(\frac{i}{n} \cdot \frac{n-i}{n} \right)^{-1}
\left| \left| W_j \left( \frac{i}{n} \right) \right| \right|^2_2,
\end{equation}
which is the maximum of the squared $L_2$ norm of the empirical fluctuation process 
scaled by its variance function. This is the sup$LM$ statistic of \cite{Andrews1993}
which can be interpreted as the supremum of $LM$ statistics against a single change point alternative
where the potential change point is shifted over the interval $[\ui, \oi]$ that
is typically defined by requiring some minimal segment size $\ui$ and then
$\oi = n - \ui$.
This test statistic is asymptotically equivalent to the supremum of likelihood-ratio (or Chow)
statistics \citep{Chow1960} but has the advantage that the model has to be fitted
only once under the null hypothesis of parameter stability and not under the 
alternative for each conceivable change point.
The limiting distribution of the sup$LM$ statistic is given by the supremum of a
squared, $k$-dimensional tied-down Bessel process $\sup_t (t (1-t))^{-1} ||W^0(t)||_2^2$
from which the corresponding $p$~value $p_j$ can be computed \citep{Hansen1997}
for the ordering $Z_j$ ($j = 1, \dots, \ell$).
To avoid that the test becomes liberal, it can be imposed---in addition to the
minimal segment size---that in each node at least a certain fraction of the current
observations are trimmed on each end. Typically, 10\% are used for this \citep{Andrews1993}.
The trimming affects only the tests not the subsequent splitting.

This approach of assessing numeric partitioning variables combines the ideas from
(linear) model tree algorithms, such as GUIDE \citep{Loh2002} or the RD and RA trees of
\cite{PottsSammut2005}, with state-of-the-art methodology for testing parameter
instability \citep{Andrews1993,Zeileis2005}. Both GUIDE and RD/RA trees assess
the parameter instability along numerical partitioning variables. The tests
employed in these algorithms are based on some ad-hoc approximations:
GUIDE and RD trees evaluate only a fixed number of conceivable change points, such as
3 (for GUIDE) or 5 (for RD trees) points at sample quantiles. Furthermore, GUIDE and RA trees
assess only the sign of the residuals, not the full model scores. Finally, RD
trees employ a crude Bonferroni approximation for computing the $p$~values instead
of the correct limiting distribution. Thus, we start from the same ideas but embed
them into a state-of-the-art framework for testing structural stability in general
parametric models. For the linear regression model, the tests employed by GUIDE and
the RD/RA trees are contained in our framework as special cases via the choise of
a different aggregation functional $\lambda(\cdot)$. As mentioned above, any (reasonable)
functional $\lambda$ could be used, because these always lead to omnibus tests
that are not uniformly dominated by any other functional over all conceivable patterns
of parameter changes. However, the $\lambda_{\sup LM}$ is particularly
attractive for fitting tree models because it has high power against abrupt changes
\citep{AndrewsPloberger1994} as captured by partitioning, evaluates all
conceivable change points, and is sensitive to changes in all elements of the parameter
vector $\theta$. The latter is not true for residual-based tests as employed by GUIDE;
this is illustrated in Section~\ref{sec:discussion}.


%% Cramer-von Mises statistic
%% To assess the instability with respect to a numerical variable $Z_j$, we suggest
%% the following: order the observations by increasing values of $Z_j$, compute the
%% fluctuation process from Equation~(\ref{eq:Wj}), and capture its fluctuation
%% using a Cram\'er-von Mises statistic:
%% 
%% \begin{equation} \label{eq:CvM}
%% \lambda_{\mbox{CvM}} (W_j) \quad = \quad
%% \frac{1}{n} \sum_{i = 1}^n \left| \left| W_j \left( \frac{i}{n} \right) \right| \right|^2_2.
%% \end{equation}
%% 
%% The asymptotic distribution of
%% $\lambda_{\mbox{CvM}} (W_j)$ is given by $\int_0^1 \left| \left| W^0(t) \right| \right|^2_2 \mbox{ d} t$
%% from which the corresponding $p$~value $p_j$ can be computed
%% \citep{ZeileisHornik2007}.


\textbf{Assessing categorical variables:}
To capture the instability with respect to a categorical variable $Z_j$ with $C$ different
levels or categories, a different statistic is required because, by definition, $Z_j$
has ties and hence a total ordering of the observations is not possible.
The most natural statistic, which is insensitive to the ordering of the $C$ levels and
the ordering of observations within each level, is given by

\begin{equation} \label{eq:Chisq}
\lambda_{\chi^2} (W_j) \quad = \quad
\sum_{c = 1}^C \frac{\left| I_c \right|}{n}^{-1} 
\left| \left| \Delta_{I_c} W_j \left( \frac{i}{n} \right) \right| \right|^2_2
\end{equation}

where $\Delta_{I_c} W_j$ is the increment of the empirical fluctuation process
over the observations in category $c = 1, \dots, C$ (with associated indexes $I_c$),
i.e., essentially the sum of the scores in category $c$. The test statistic is then
the weighted sum of the squared $L_2$ norm of the increments
which has an asymptotic $\chi^2$ distribution with $k \cdot (C-1)$ degrees of freedom from
which the corresponding $p$~value $p_j$ can be computed \citep{HjortKoning2002}.

The advantage of using this approach, based on the empirical fluctuation processes
from Equation~\ref{eq:Wj} with the functionals from Equations~\ref{eq:supLM} and
\ref{eq:Chisq}, is that the parameter estimates and corresponding score functions
just have to be computed once in a node. For performing the parameter instability tests,
the scores just have to be reordered and aggregated to a scalar test statistic each time. 

To test whether there is some overall instability in the current node, it just has
to be checked whether the minimal $p$~value $\min_{j = 1, \dots, \ell} p_j$ falls below
a pre-specified significance level $\alpha$, which is typically corrected for multiple
testing. If this is the case, the variable $Z_{j^*}$ associated with the minimal $p$~value
is chosen for splitting the model in the next step of the algorithm.

\subsection{Splitting}

In this step of the algorithm the fitted model has to be split with respect to the
variable $Z_{j^*}$ into a segmented model
with $B$ segments where $B$ can either be fixed or determined adaptively. For a fixed
number of splits, two rival segmentations can be compared easily by comparing the
segmented objective function $\sum_{b = 1}^B \sum_{i \in I_b} \Psi(Y_i, \theta_b)$.
Performing an exhaustive search over all conceivable partitions with $B$ segments
is guaranteed to find the optimal partition but might be burdensome, so several search
methods are briefly discussed for numerical and categorical partitioning variables 
respectively.

\textbf{Splitting numerical variables:}
Exhaustive search for a split into $B = 2$ segments is feasible in
$O(n)$ operations. For $B > 2$, an exhaustive search would be of order
$O(n^{B-1})$---however, the optimal partition can be found using a dynamic
programming approach of order $O(n^2)$. This is an application of Bellman's principle and has been
discussed in several places in the literature on
change point and structural change analysis
\citep[see e.g.,][among others]{Hawkins2001,BaiPerron2003,ZeileisKleiberKraemer2003}.
Alternatively, iterative algorithms can be used that are known to converge to
the optimal solution \citep[e.g.,][]{Muggeo2003}. If $B$ is not fixed, but 
should be chosen adaptively, various methods are available
\citep[see e.g.,][]{BaiPerron2003,OBrien2004}. In particular, information
criteria can be used if the parameters are estimated by ML.

\textbf{Splitting categorical variables:}
For categorical variables, the number of segments can not be larger than
the number of categories $B \le C$. Two simple approaches would be either to always
split into all $B = C$ possible levels or alternatively to always split into
the minimal number of $B = 2$ segments. In the latter case, the search for the optimal
partition is of order $O(2^{C-1})$. For ordinal variables, it also makes sense
to just split in the ordering of the levels, so that the search for a binary
split is only of order $O(C)$. Again, information criteria could be an option
to adaptively determine the number of splits, although this is less intuitive
than for numerical variables.

In summary, two plausible strategies would be either to always use binary splits, i.e.,
use a fixed $B = 2$, or to determine $B$ adaptively for numerical variables while
always using $B = C$ for categorical variables. In Section~\ref{sec:illustration}
below, we adopt the former strategy of binary splits.

This concludes one iteration of the recursive partitioning algorithm and steps~1--3
are carried out again in each of the $B$ daughter nodes until no significant 
instability is detected in step~2.

\section{Illustrations and applications}
\label{sec:illustration}

<<results,echo=FALSE,results=hide>>=
if(file.exists("benchmarks/MOB-fit.rda") && file.exists("benchmarks/results.rda")) {
  load("benchmarks/MOB-fit.rda")
  load("benchmarks/results.rda")
} else {
  setwd("benchmarks/")
  source("results.R")
  setwd("../")
}

source("benchmarks/perfplot.R")

JournalsResults <- cbind(as.vector(names(JournalsRMSE)),
  format(round(apply(JournalsRMSE, 2, median), digits = 3), nsmall = 3),
  format(round(JournalsObvious[1,], digits = 3), nsmall = 3),
  round(apply(JournalsNPAR, 2, median), digits = 0),
  JournalsObvious[2,])

BostonHousingResults <- cbind(as.vector(names(BostonHousingRMSE)),
  format(round(apply(BostonHousingRMSE, 2, median), digits = 3), nsmall = 3),
  format(round(BostonHousingObvious[1,], digits = 3), nsmall = 3),
  round(apply(BostonHousingNPAR, 2, median), digits = 0),
  BostonHousingObvious[2,])

PID_Results <- cbind(as.vector(names(PID_MC)),
  format(round(apply(PID_MC, 2, median), digits = 3), nsmall = 3),
  format(round(PID_Obvious[1,], digits = 3), nsmall = 3),
  round(apply(PID_NPAR, 2, median), digits = 0),
  PID_Obvious[2,])


ntJ <- NROW(coef(fmJ))
nkJ <- NCOL(coef(fmJ))
npJ <- ntJ * nkJ + ntJ - 1
ntBH <- NROW(coef(fmBH))
nkBH <- NCOL(coef(fmBH))
npBH <- ntBH * nkBH + ntBH - 1
ntPID <- NROW(coef(fmPID))
nkPID <- NCOL(coef(fmPID))
npPID <- ntPID * nkPID + ntPID - 1
@

To illustrate how model-based recursive partitioning can be used in practice,
the general framework from the previous section is applied to various 
regression problems (linear regression, logistic regression, and survival regression).
Four different data sets are analyzed in the following way: In a first
step, the recursively partitioned model is fitted to the data and visualized for
explanatory analysis, emphasizing that the algorithm can be used to build intelligible
local models by automated interaction detection.
In a second step, the performance of the algorithm is compared
with other tree-based algorithms in two different respects: prediction and complexity. 
Comparing predictive performance of different learning algorithms is established
practice---for (model-based) recursive partitioning comparing the model complexity (i.e.,
the number of splits and estimated coefficients) is equally important. As argued
above, the strength of single tree-based classifiers is not so much predictive
power alone, but that the algorithms are able to build interpretable models. Clearly,
more parsimonious models are easier to interpret and hence are to be preferred (among those
with comparable predictive performance).

For the linear regression applications, the \underline{mo}del-\underline{b}ased (MOB)
recursive partitioning algorithm introduced here is compared to other algorithms 
previously suggested in the literature: GUIDE \citep{Loh2002} and
M5' \citep{WangWitten1997}, which is a rational reconstruction of M5 \citep{Quinlan1992},
as linear model trees; as well as CART \citep[classification and regression trees,][]{Breimanetal1984}
and conditional inference trees \citep[CTree,][]{HothornHornikZeileis2006} as trees with
constant models in the nodes. The logistic regression-based MOB trees are compared
with logistic model trees \citep[LMT,][]{Landwehretal2005} as well as various tree-based algorithms
with constant models in the nodes: QUEST \citep{LohShih1997}, CRUISE \citep{KimLoh2001},
the J4.8 implementation \citep{WittenFrank2005} of C4.5 \citep{Quinlan1993}, CART and CTree.

All benchmark comparisons are carried out in the framework of \cite{HothornLeischZeileis2005}
based on 250 bootstrap replications and employing the root mean squared error (RMSE)
or misclassification rate on the out-of-bag (OOB) samples as predictive
performance measure and the number of estimated parameters (splits and coefficients) as
complexity measure.
The median performances on the bootstrap replications are reported in tabular form,
simultaneous confidence intervals for performance differences (obtained by treatment
contrasts with MOB as the reference category) are visualized. The median rather than
the mean performance is used for two reasons: First, to account for the skewness of
the performance distributions, and second due to the many ties
in the complexity measure for the model-based tree algorithms (MOB, GUIDE, M5', LMT).
In addition to the OOB performances, the tables contain the obvious complexity and prediction
performance measures (RMSE or misclassification) on the original data set as an additional
reference information (although the obvious prediction measures obviously represent no honest estimators).

Most computations have been carried out in the \proglang{R} system for statistical computing
\citep{R2007}, in particular using the packages \pkg{party} \citep{HothornZeileisHornik2006a},
providing implementations of MOB and CTree, \pkg{rpart} \citep{TherneauAtkinson1997}, implementing
CART, and \pkg{RWeka} \citep{HornikZeileisHothorn2006}, the \proglang{R} interface to \pkg{Weka}
\citep{WittenFrank2005} containing implementations of M5', LMT and J4.8. For GUIDE, QUEST and CRUISE,
the binaries distributed at \url{http://www.stat.wisc.edu/~loh/} were used.

\subsection{Demand for economic journals}

Journal pricing is a topic that stirred considerable interest in the economics literature in
recent years, see \cite{Bergstrom2001} and his journal pricing Web page
\url{http://www.econ.ucsb.edu/~tedb/Journals/jpricing.html} for further informations on this 
discussion. Using data collected by T.~Bergstrom for $n = 180$ economic journals,
\cite{StockWatson2003} fit a demand equation by OLS
for the number of library subscriptions explained by the price per citation (both in logs).
In their analysis, they find that this simple linear regression can be improved by including
further variables such as age, number of characters and interactions of age and price into 
the model with no clear solution what is the best way of incorporating these further variables.

This is where we set out with an analysis by means of model-based recursive partitioning.
The model to be partitioned is a linear regression for the number of library subscriptions by
price per citation in log-log specification (i.e., with $k = 2$ coefficients).
The $\ell = 5$ partitioning variables are the raw price
and number of citations, the age of the journal, number of characters and a factor indicating whether the
journal is associated with a society or not. Thus, we use a standard model whose specification
is driven by economic knowledge and try to partition it with respect to further variables
whose influence is not clear in advance. Note that whereas the selection of appropriate transformations
is crucial for the modeling variables, monotone transformations of the partitioning
variables have no influence on the fitting process. For testing, a Bonferroni-corrected
significance level of $\alpha = 0.05$ and a minimal segment size of $\ui = 10$ are used.

\setkeys{Gin}{width=.78\textwidth}
\begin{figure}[p]
\begin{center}
<<Journals-plot,echo=FALSE,results=hide,fig=TRUE,height=5,width=8.2>>=
plot(fmJ, tp_args = list(col = grey(0.5), linecol = "black", xlab = "log(price/citation)",
  ylab = "log(subscriptions)"))
@
\caption{\label{fig:Journals} Linear-regression-based tree for the economic journals data.
The plots in the leaves depict library subscriptions by price per citation (both in logs).}
\end{center}
\end{figure}

\begin{table}[p]
\begin{center}
\begin{tabular}{|r|rr||rrrrr|} \hline
Node & \multicolumn{2}{|c||}{Regressors} & \multicolumn{5}{|c|}{Partitioning variables} \\
 & (Intercept) & log(price/citation) & price & citations & age & chars & society \\ \hline
<<Journals-algorithm,echo=FALSE,results=tex>>=
stable <- function(i) {
  x <- cbind(t(summary(fmJ, node = i)$coefficients[,c(1, 4)]), sctest(fmJ, node = i)[,c(5, 2:4, 1)])
  paste(i, "&", paste(paste("$", format(c(0.001, round(x[1,], digits = 3)))[-1], "$", sep = ""),
    collapse = " & "), "\\\\", "\n", "&", paste(sapply(x[2,],  function(p, digits = 3)
    ifelse(p <= 10^(-digits), paste("< 0.", paste(rep("0", digits-1), collapse = "") , "1", sep = ""),
    paste("", format(c(round(p, digits = digits), 0.001)), "", sep = "")[1])), collapse = " & "),
    "\\\\", "\\hline", "\n")
}
cat(stable(1))
cat(stable(2))
cat(stable(3))
@
\end{tabular}
\caption{\label{tab:Journals-algorithm} Summary of the fitting process for the linear-regression-based
tree for the economic journals data. The first two column summarize the regressors in the linear
regression by means of estimated coefficients and associated Wald test $p$~values. The remaining
five columns summarize the partitioning variables by means of parameter instability statistics and
associated
$p$~values.}
\end{center}
\end{table}

\begin{table}[p]
\begin{center}
\begin{tabular}{|r|rr|rr|} \hline
 & \multicolumn{2}{|c|}{RMSE} & \multicolumn{2}{|c|}{Number of parameters} \\
 & Bootstrap & Original & Bootstrap & Original \\ \hline
<<Journals-results,echo=FALSE,results=tex>>=
cat(paste(apply(JournalsResults, 1, function(x) paste(paste(x, collapse = " & "), "\\\\")), collapse = "\n"))
@

\hline
\end{tabular}
\caption{\label{tab:Journals} Performance comparison for economic journals data:
prediction error is compared by median root mean squared error (RMSE) on 250
bootstrap samples and obvious RMSE on the original data set; complexity
is compared by (median) number of estimated parameters.}
\end{center}
\end{table}


The resulting linear regression-based tree for the economic journals data is depicted in
Figure~\ref{fig:Journals} employing scatter plots with fitted regression lines in the leaves;
some more details are provided in Table~\ref{tab:Journals-algorithm}.
In the fitting process, a global model for all observations is estimated in node~1 yielding a price
elasticity of about $\Sexpr{round(coef(fmJ, node = 1)[2], digits = 2)}$. Its stability is
assessed with respect to all $\ell = 5$ partitioning variables, the corresponding
parameter instability test statistics (as defined in Equation~\ref{eq:supLM} and
\ref{eq:Chisq}, respectively) are provided in Table~\ref{tab:Journals-algorithm}
along with their Bonferroni-adjusted $p$~values. A highly significant instability is
found only with respect to age (with a Bonferroni-adjusted $p$~value of
$\Sexpr{Pval(sctest(fmJ, node = 1)[2,3])}$) which is subsequently used for splitting, 
leading to an optimal split at age 18.
For the \Sexpr{sum(weights(fmJ@tree$left$model))} young journals in node~2
a much higher price elasticity of about $\Sexpr{round(coef(fmJ, node = 2)[2], digits = 2)}$
is found than for the \Sexpr{sum(weights(fmJ@tree$right$model))} older journals in node~3
with a price elasticity of about $\Sexpr{round(coef(fmJ, node = 3)[2], digits = 2)}$.
No further parameter instabilities with respect to the partitioning variables can be detected:
all $p$~values are greater than 89\% and hence the algorithm stops.
Table~\ref{tab:Journals} reports the RMSE on the original data and the model complexity of
\Sexpr{npJ} (\Sexpr{ntJ} times $k = \Sexpr{nkJ}$ coefficients plus $\Sexpr{ntJ} - 1$ splits).

Table~\ref{tab:Journals} and Figure~\ref{fig:Journals-performance} provide the results
of the benchmark comparison on 250 bootstrap samples. The MOB trees have the lowest median
RMSE and complexity, the simultaneous confidence intervals show that the differences compared
to GUIDE are non-significant in both cases and significant compared to all other models.
While the trees with constant fits are clearly outperformed with respect to RMSE, M5' is
still quite close in terms of RMSE but requires a substantially larger number of parameters
to achieve this predictive performance.

\setkeys{Gin}{width=\textwidth}
\begin{figure}[t!]
\begin{center}
<<Journals-performance,echo=FALSE,results=hide,fig=TRUE,height=5,width=10,twofig=TRUE>>=
ciplot(JournalsRMSE, xlab = "RMSE difference")
ciplot(JournalsNPAR, xlab = "Complexity difference")
@
\caption{\label{fig:Journals-performance} Performance comparison for economic journals data: 
prediction error is compared by RMSE differences, 
complexity by difference in number of estimated parameters (coefficients and split points).}
\end{center}
\end{figure}

\subsection{Boston housing data}

Since the analysis by \cite{BreimanFriedman1985}, the Boston housing data are 
a popular and well-investigated empirical basis for illustrating non-linear 
regression methods both in machine learning and statistics
\citep[see][for two recent examples]{Gama2004,Samarovetal2005} and we follow 
these examples by segmenting a bivariate linear regression model for the house
values.

\setkeys{Gin}{width=\textwidth}
\begin{figure}[p]
\begin{center}
<<BostonHousing-plot,echo=FALSE,results=hide,fig=TRUE,height=8,width=12>>=
plot(fmBH, terminal_panel = node_scatterplot(fmBH, col = grey(0.5), linecol = "black"), tnex = 4)
@
\caption{\label{fig:BostonHousing} Linear-regression-based tree for the Boston housing data.
The plots in the leaves give partial scatter plots for $\log(\mbox{lstat})$ (upper panel) and 
$(\mathrm{rm})^2$ (lower panel).}
\end{center}
\end{figure}

\setkeys{Gin}{width=\textwidth}
\begin{figure}[p]
\begin{center}
<<BostonHousing-performance,echo=FALSE,results=hide,fig=TRUE,height=5,width=10,twofig=TRUE>>=
ciplot(BostonHousingRMSE, xlab = "RMSE difference", xlim = c(-0.05, 1))
ciplot(BostonHousingNPAR, xlab = "Complexity difference")
@
\caption{\label{fig:BostonHousing-performance} Performance comparison for Boston housing data: 
prediction error is compared by RMSE differences, complexity by difference in number of
estimated parameters.}
\end{center}
\end{figure}

The data set provides $n = 506$ observations of the median value of owner-occupied
homes in Boston (in USD~1000) along with $14$ covariates including in particular
the number of rooms per dwelling (rm) and the percentage
of lower status of the population (lstat). A segment-wise linear relationship between
the value and these two variables is very intuitive, whereas the shape of the influence
of the remaining covariates is rather unclear and hence should be learned from the data.
Therefore, a linear regression model for median value explained by $(\mathrm{rm})^2$
and $\log(\mbox{lstat})$ with $k = 3$ regression coefficients is employed and
partitioned with respect to all $\ell = 11$ remaining variables. As argued above,
choosing appropriate transformations of the modeling variables
is important to obtain a well-fitting model in each segment and
we follow in our choice the recommendations of \cite{BreimanFriedman1985}.
The model is estimated by OLS, the instability is assessed using a Bonferroni-corrected
significance level of $\alpha = 0.05$ and the nodes are split with a required minimal
segment size of $\ui = 40$.

The resulting model-based tree is depicted in Figure~\ref{fig:BostonHousing} which
shows partial scatter plots along with the fitted values in the terminal nodes. It can
be seen that in the nodes~3, 6 and 7 the increase of value with the number of rooms
dominates the picture (lower panel) whereas in node~9 the decrease with
the lower status population percentage (upper panel) is more pronounced. Splits are performed
in the variables  tax (property-tax rate) and ptratio (pupil-teacher ratio).
As reported in Table~\ref{tab:BostonHousing},
the model has $\Sexpr{ntBH} \cdot \Sexpr{nkBH}$ regression coefficients after estimating 
$\Sexpr{ntBH}-1$ splits, giving
a total of $\Sexpr{(nkBH+1)*ntBH - 1}$ estimated parameters.

The results of the benchmark comparison in Table~\ref{tab:BostonHousing} and
Figure~\ref{fig:BostonHousing-performance} show that the MOB trees perform significantly better
on this data set than the other tree-based algorithms. The algorithm with the most
comparable predictive performance, M5', is here clearly inferior concerning its interpretability,
requiring on average more than 12 times as many parameters.

\begin{table}[t!]
\begin{center}
\begin{tabular}{|r|rr|rr|} \hline
 & \multicolumn{2}{|c|}{RMSE} & \multicolumn{2}{|c|}{Number of parameters} \\
 & Bootstrap & Original & Bootstrap & Original \\ \hline
<<BostonHousing-results,echo=FALSE,results=tex>>=
cat(paste(apply(BostonHousingResults, 1, function(x) paste(paste(x, collapse = " & "), "\\\\")), collapse = "\n"))
@

\hline
\end{tabular}
\caption{\label{tab:BostonHousing} Performance comparison for Boston housing data:
prediction error is compared by RMSE on 250 bootstrap samples and obvious RMSE on
the original data set; complexity is compared by (median) number of estimated parameters.}
\end{center}
\end{table}



\subsection{Pima Indians diabetes data}
 
\begin{table}[b!]
\begin{center}
\begin{tabular}{|r|rr|rr|} \hline
 & \multicolumn{2}{|c|}{Misclassification} & \multicolumn{2}{|c|}{Number of parameters} \\
 & Bootstrap & Original & Bootstrap & Original \\ \hline
<<PID-results,echo=FALSE,results=tex>>=
cat(paste(apply(PID_Results, 1, function(x) paste(paste(x, collapse = " & "), "\\\\")), collapse = "\n"))
@

\hline
\end{tabular}
\caption{\label{tab:PID} Performance comparison for Pima Indians data:
prediction error is compared by misclassification rate on 250 bootstrap samples and obvious
misclassification on the original data set; complexity is compared by (median) number of
estimated parameters.}
\end{center}
\end{table}

\setkeys{Gin}{width=.95\textwidth}
\begin{figure}[p]
\begin{center}
<<PID-plot,echo=FALSE,results=hide,fig=TRUE,height=6,width=10>>=
plot(fmPID, terminal_panel = node_bivplot(fmPID, gp = gpar(fill = grey(c(0.5, 0.8))),
  linecol = "black", ylines = 1.5, ylab_tol = 1))
@
\caption{\label{fig:PID} Logistic-regression-based tree for the Pima Indians data.
The spinograms in the leaves depict diabetes by plasma glucose concentration.}
\end{center}
\end{figure}

\setkeys{Gin}{width=\textwidth}
\begin{figure}[p]
\begin{center}
<<PID-performance,echo=FALSE,results=hide,fig=TRUE,height=5,width=10,twofig=TRUE>>=
ciplot(PID_MC, xlab = "Misclassification difference", xlim = c(-0.005, 0.05))
ciplot(PID_NPAR, xlab = "Complexity difference")
@
\caption{\label{fig:PID-performance} Performance comparison for Pima Indians data: 
prediction error is compared by misclassification rate differences, 
complexity by difference in number of estimated parameters.}
\end{center}
\end{figure}

Another popular data set for comparing new classifiers is the Pima Indians diabetes
data which is---just as the Boston Housing data---available from the 
UCI machine learning repository \citep{Newmanetal1998}. The data set contains
many missing values---usually falsely coded as (physically impossible) zero values---which
are most prevalent in the variables serum insulin and triceps skin fold thickness
\citep{Ripley1996}. Hence, these two variables and the missing values in the remaining
data are omitted, so that the data comprises 
observations for $n = 724$ Pima Indian women of 6 prognostic variables
and the outcome (positive/negative) of a diabetes test. It is rather clear
that the diabetes diagnosis depends on the plasma glucose concentration such that
using a logistic regression model for diabetes explained by glucose
(corresponding to $k = 2$ parameters) is intuitive. This model is partitioned with
respect to the remaining $\ell = 5$ variables, using a minimal segment size of
$\ui = 40$ and again a Bonferroni-corrected significance level of $\alpha = 0.05$.

Figure~\ref{fig:PID} displays the resulting logistic regression-based tree. The
data is first split at a body mass index of 26.3 (corresponding roughly to the lower 
quartile of this variable), those observations with a higher body mass index are
partitioned into age groups below or above 30 years. The leaves of the tree visualize
the data and the fitted logistic regression model using spinograms \citep{HofmannTheus2005} 
of diabetes by glucose (where the bins are chosen via the five point summary of
glucose on the full data set). It can be seen that for women with a low body mass 
index the average risk of diabetes is low, but increases clearly with age (corresponding
to an odds ratio of $\Sexpr{format(round(exp(coef(fmPID)[1,2]), digits = 3), nsmall = 3)}$ per glucose unit).
For the young women with a high body mass index, the average risk is higher and increases
less quickly with respect to age (with an odds ratio of
$\Sexpr{format(round(exp(coef(fmPID)[2,2]), digits = 3), nsmall = 3)}$). Finally, the older
women with a high body mass index have the highest average risk but with a lower
odds ratio of only $\Sexpr{format(round(exp(coef(fmPID)[3,2]), digits = 3), nsmall = 3)}$.
The model uses $\Sexpr{npPID}$ parameters ($\Sexpr{ntPID} \cdot \Sexpr{nkPID}$ coefficients
and $\Sexpr{ntPID} - 1$ splits).

The results of the benchmark comparison in Table~\ref{tab:PID} and
Figure~\ref{fig:PID-performance} show that the MOB trees perform slightly (and significantly) better
on this data set than the other tree-based algorithms included. In particular, it performs
considerably better than the other model-based algorithm (LMT) both with respect to prediction
and model complexity.



\subsection{German breast cancer study}

The same ideas used for recursive partitioning in (generalized) linear regression
models can straightforwardly be applied to other parametric regression models
without further modification. Here, we apply the generic model-based recursive
partitioning algorithm to a Weibull regression for modeling censored survival
times. We follow the analysis of \cite{Schumacheretal2001} and
\cite{HothornHornikZeileis2006}
who use constant fit survival trees to analyze survival times of $n = 686$ women
from positive node breast cancer in Germany. Along with the survival time (in years)
and the censoring information, there are 8 covariates available as prognostic factors:
number of positive lymph nodes, age, tumor size and grade, progesterone and
estrogen receptor, and factors indicating menopausal status and whether the patient
received a hormonal therapy.

For explaining survival from positive node breast cancer in a regression model,
the number of positive lymph nodes is chosen as the explanatory variable with differing
intercepts depending on whether a hormonal therapy was performed or not. Together
with the scale parameter of the Weibull distribution, this gives a total of
$k = \Sexpr{NCOL(vcov(fmGBSG2@tree$model))}$ parameters in the model, using the 
remaining $\ell = 6$ prognostic variables for partitioning.
The model is estimated by ML, the instability is assessed using a Bonferroni-corrected
significance level of $\alpha = 0.05$ and the nodes are split with a required minimal
segment size of $\ui = 40$.

\setkeys{Gin}{width=.8\textwidth}
\begin{figure}[t!]
\begin{center}
<<GBSG2-plot,echo=FALSE,results=hide,fig=TRUE,height=5,width=8.5>>=
node_gbsg2plot <- function(mobobj, col = "black", linecol = "red",
  cex = 0.5, pch = NULL, jitter = FALSE, xscale = NULL, yscale = NULL, ylines = 1.5,
  id = TRUE, xlab = FALSE, ylab = FALSE)
{
    ## extract dependent variable
    y <- response(mobobj)
    ynam <- names(y)[1]
    y <- y[[1]]
    if(is.null(pch)) pch <- y[,2] * 18 + 1 #reverse:# abs(y[,2] - 1) * 7 + 1
    y <- y[,1]
    y <- as.numeric(y)
    pch <- rep(pch, length.out = length(y))

    if(jitter) y <- jitter(y)

    ## extract regressor matrix
    x <- mobobj@data@get("input")
    xnam <- colnames(x)
    z <- seq(from = min(x[,2]), to = max(x[,2]), length = 51)
    z <- data.frame(a = rep(sort(x[,1])[c(1, NROW(x))], c(51, 51)), b = z)
    names(z) <- names(x)
        
    if(is.null(xscale)) xscale <- range(x[,2]) + c(-0.1, 0.1) * diff(range(x[,2]))
    if(is.null(yscale)) yscale <- range(y) + c(-0.1, 0.1) * diff(range(y))
         
    ## panel function for scatter plots in nodes
    rval <- function(node) {
    
        ## dependent variable setup
	y <- rep.int(y, node$weights)	
	yhat <- predict(node$model, newdata = z, type = "quantile", p = 0.5)
        pch <- rep.int(pch, node$weights)

        ## viewport setup
        top_vp <- viewport(layout = grid.layout(nrow = 2, ncol = 3,
                           widths = unit(c(ylines, 1, 1), c("lines", "null", "lines")),  
			   heights = unit(c(1, 1), c("lines", "null"))),
                           width = unit(1, "npc"), 
                           height = unit(1, "npc") - unit(2, "lines"),
			   name = paste("node_scatterplot", node$nodeID, sep = ""))
        pushViewport(top_vp)
        grid.rect(gp = gpar(fill = "white", col = 0))

        ## main title
        top <- viewport(layout.pos.col = 2, layout.pos.row = 1)
        pushViewport(top)
	mainlab <- paste(ifelse(id, paste("Node", node$nodeID, "(n = "), ""),
	                 sum(node$weights), ifelse(id, ")", ""), sep = "")
        grid.text(mainlab)
        popViewport()
	
        plot_vp <- viewport(layout.pos.col = 2, layout.pos.row = 2, xscale = xscale,
	    yscale = yscale, name = paste("node_scatterplot", node$nodeID, "plot", sep = ""))
        pushViewport(plot_vp)
	
        ## scatterplot
	grid.points(rep.int(x[,2], node$weights), y, gp = gpar(col = col, cex = cex), pch = pch)
        grid.lines(z[1:51,2], yhat[1:51], default.units = "native", gp = gpar(col = linecol))
        grid.lines(z[52:102,2], yhat[52:102], default.units = "native", gp = gpar(col = linecol, lty = 2))

        grid.xaxis(at = c(ceiling(xscale[1]*10), floor(xscale[2]*10))/10)
        grid.yaxis(at = c(ceiling(yscale[1]), floor(yscale[2])))

        if(isTRUE(xlab)) xlab <- xnam[2]
	if(isTRUE(ylab)) ylab <- ynam
        if(!identical(xlab, FALSE)) grid.text(xlab, x = unit(0.5, "npc"), y = unit(-2, "lines"))
        if(!identical(ylab, FALSE)) grid.text(ylab, y = unit(0.5, "npc"), x = unit(-2, "lines"), rot = 90)

        grid.rect(gp = gpar(fill = "transparent"))
        upViewport()

	upViewport()
    }
	    
    return(rval)
}
class(node_gbsg2plot) <- "grapcon_generator"

plot(fmGBSG2, terminal_panel = node_gbsg2plot, tp_args = list(yscale = c(-0.15, 8.7),
  ylines = 2.5, xscale = c(0, 52), col = grey(0.5), linecol = "black",
  xlab = "Number of positive lymph nodes", ylab = "Survival time"))
@
\caption{\label{fig:GBSG2} Weibull-regression-based tree for the German breast cancer study.
The plots in the leaves depict censored (hollow) and uncensored (solid) survival time by number
of positive lymph nodes along with fitted median survival for patients with (dashed line) and without
(solid line) hormonal therapy.}
\end{center}
\end{figure}

The resulting model-based tree is depicted in Figure~\ref{fig:GBSG2} employing
scatter plots for survival time by number of positive nodes in the leaves. Based on
the ideas of \cite{GentlemanCrowley1991}, circles with different shadings of gray 
(hollow and solid) are used for censored and uncensored observations, respectively.
Fitted median survival times from the Weibull regression model are visualized by
dashed and solid lines for patients with and without hormonal therapy. The data are 
partitioned once with respect to progesterone receptor, splitting the observations
into a group with marked influence of positive nodes and negligible influence
of hormonal therapy and a group with less pronounced influence of positive nodes but
clear hormonal therapy effect. A fair amount of censored observations remains in
both groups; no further significant instabilities can be detected (all $p$~values
are above $50\%$). The resulting model has 
$\Sexpr{ncol(vcov(fmGBSG2@tree$model)) * nrow(coef(fmGBSG2)) + nrow(coef(fmGBSG2)) - 1}$ parameters
($\Sexpr{nrow(coef(fmGBSG2))} \cdot \Sexpr{ncol(vcov(fmGBSG2@tree$model))}$ coefficients
and $\Sexpr{nrow(coef(fmGBSG2))} - 1$ splits), yielding a log-likelihood of
$\Sexpr{round(as.vector(logLik(fmGBSG2)), digits = 3)}$.

For this survival regression problem, we refrain from conducting a benchmark comparison
of performance as carried out in the previous section. The main reason for this is
that it is not clear which predictive perfomance measure should be used in such a 
comparison: Whereas RMSE and misclassification are usually regarded to be acceptable (albeit not the
only meaningful) performance measures for regression and classification tasks, the
situation is not as well understood for censored regression models. Although various
measures, such as the Brier score \citep{Grafetal1999}, 
are used in the literature their usefulness still remains a matter of debate
\citep{Henderson1995,AltmanRoyston2000,Schemper2003}. As resolving these
discussions is beyond the scope of this paper, we content ourselves 
with the empirical analysis for this regression problem.


\section{Discussion} \label{sec:discussion}

In this section, some aspects of model-based recursive partitioning are discussed
in more detail or compared with previous approaches---also, some ideas for extensions
are given.

\textbf{Parameter instability tests:} To illustrate size and power properties of the
score-based fluctuation tests employed in the MOB algorithm, we present a simple simulation study.
The results are compared to those from GUIDE which uses residual-based tests to determine
the partitioning variable in each node. A simple linear regression $y = x^\top \beta + \varepsilon$
is used where both the regressor~$x$ and the error~$\varepsilon$ are standard normal. There
are six splitting variables: three standard normal variables $Z_1$, $Z_2$, $Z_3$, one uniform
variable $Z_4$ on $[0,1]$ (rounded to 1 digit to yield many ties), one discrete uniform categorical
variable $Z_5$ with 2 categories and another variable $Z_6$ with 5 categories. We draw $n = 500$
observations from this data-generating process in three scenarios: (1)~no change, $\beta = (0, 0)^\top$
on the full sample, (2)~intercept change, $\beta$ switches to $(1, 0)^\top$ for $Z_1 > 0$, (3)~slope
change, $\beta$ switches to $(0, 1)^\top$ for $Z_1 > 0$. The outcome from both MOB (with $\alpha = 0.05$)
and GUIDE applied to $500$ samples drawn from these models are summarized in Table~\ref{tab:size-power}.
Under the null hypothesis of no change, both algorithms select almost always the right model without 
any splits. GUIDE's model search performs somewhat better---however, MOB selects splits only in
roughly 5\% of all replications, i.e., maintains its nominal size and 
works as expected and advertised. In the power case for an
intercept change, both algorithms again perform very similar and always find the correct partitioning
variable. Again, MOB selects a larger model somewhat more often, in roughly 5\% of the replications.
In the power case for a slope change, however, the GUIDE procedure breaks down while MOB performs
as for the intercept change. The reason for GUIDE's behaviour is that residual-based tests
are insensitive to parameter changes orthogonal to the mean regressor \citep{PlobergerKraemer1992}.
This results in too few or too many (up to 12) splits selected, more often than not in the 
wrong partitioning variable. Although this is an extreme scenario, similar situations can
occur in practice when the variables are standardized prior to modeling (as commonly done
for many statistical and machine learning algorithms). Therefore, employing the full model
scores for assessing parameter stability seems to be a beneficial strategy. For abrupt changes,
the tests advocated in the paper enjoy some weak optimality properties \citep{AndrewsPloberger1994}
but are generally also consistent for other patterns of parameter change.

<<size-power-simulation,echo=FALSE,results=hide>>=
if(file.exists("benchmarks/size-power.rda")) {
  load("benchmarks/size-power.rda")
} else {
  setwd("benchmarks/")
  source("size-power.R")
  setwd("../")
}
@

\begin{table}[t!]
\begin{center}
\begin{tabular}{|r|r|rrrrr|rrrrrr|} \hline
Change & Algorithm & \multicolumn{5}{|c|}{Number of splits} & \multicolumn{6}{|c|}{First split (if any) in} \\
       &      & 0 & 1 & 2 & 3 & 4+ & $Z_1$ & $Z_2$ & $Z_3$ & $Z_4$ & $Z_5$ & $Z_6$ \\ \hline
<<size-power,echo=FALSE,results=tex>>=
tab <- cbind(
  c("     none", "         ", "intercept", "         ", "    slope", "         "),
  rep(c("  MOB", "GUIDE"), 3),
  t(sapply(c(sim00, sim10, sim01),
           function(x) table(cut(as.numeric(as.character(x$nsplit)), c(0:4 - 0.5, 20))))),
  t(sapply(c(sim00, sim10, sim01),
           function(x) table(cut(as.numeric(as.character(x$firstvar)), 1:7 - 0.5))))
)
tab <- apply(tab, 1, function(x) paste(x, collapse = " & "))
tab <- paste(as.vector(tab), rep(c("\\\\", "\\\\ \\hline"), 3))
writeLines(tab)
@
\end{tabular}
\caption{\label{tab:size-power} Number of splits and first partitioning variable chosen for splitting
(if any) in $500$~replications for three artificial types of parameter changes:
no/intercept/slope change. }
\end{center}
\end{table}


\textbf{Unbiasedness:}
A desirable property which has been emphasized for many of the more
recent tree algorithms---such as QUEST \citep{LohShih1997}, GUIDE
\citep{Loh2002} or CTree \citep{HothornHornikZeileis2006}---is \emph{unbiasedness}. While
exhaustive search algorithms (such as CART or C4.5) have a variable
selection bias (e.g., towards partitioning variables with many potential change
points), this problem can be overcome by separating variable and split
point selection (or assessing the significance of a selected split
appropriately). As MOB is based on formal parameter instability tests,
unbiasedness is also achieved: If the model fitted in a certain node
is stable, any of the partitioning variables will only be selected
with (approximate) probability $\alpha$ because the overall test
is constructed by Bonferroni-adjusting asymptotic $p$~values from the
individual parameter instability tests for each partitioning variable.
If there is parameter instability with respect to one partitioning variable,
it will be picked up (for large enough $n$) because all tests used are
consistent \citep[at rate $\sqrt{n}$, see][]{ZeileisHornik2007}.

\textbf{Pruning:}
The algorithm as discussed in this paper relies on a statistically motivated
internal stopping criterion (sometimes called \textit{pre-pruning}). This has the
advantage that it is easy to understand and interpret the tree-growing algorithm
as it simply relies on significance tests. By construction, splits in irrelevant partitioning
variables are selected in each node only with probability $\alpha$
(as also illustrated in Table~\ref{tab:size-power}). Across nodes, significance
is controlled by an approach corresponding to closed testing procedures for
multiple comparisons \citep{MarcusPeritzGabriel1976,HochbergTamhane1987}: because the
hypotheses in a tree are recursively nested, significance of $p$~values is only interpretable if
parameter stability has been rejected for all previous nodes in the tree. Conversely,
this implies that the algorithm stops growing the tree in nodes where parameter stability cannot
be rejected. However, such an inference-based strategy does not have uniform power
against all patterns of parameter instability and it might miss certain instabilities,
e.g., interaction effects (see also below). Thus, if the focus of application is predicitve
performance, the power could be enhanced by combining the algorithm
with a refined model-selection strategy, such as cross-validation-based \textit{post-pruning}.
Note that in such a situation $p$~values can only be interpreted in an exploratory manner.
As every node of the tree is associated with a fitted model with a certain number
of parameters, another attractive option is to grow the tree with a large $\alpha$
(leading to a large tree) and then prune based on information criteria.

\textbf{Objective function:}
For some statistical models, there is a clearly defined estimating function
$\psi(Y, \theta)$ but the antiderivate $\Psi(Y, \theta)$ does not necessarily exist.
Such models can also be recursively partitioned: the parameter instability
tests work in the same way, only the selection of the splits has to be 
adapted. Instead of minimizing an objective function, the corresponding $B$-sample
split statistics have to be maximized.

\textbf{Interactions:}
Typically, recursive partitioning algorithms use perpendicular splits, i.e., the
partitioning variables $Z_j$ just include `main effects'. To prevent that the algorithm
fails to pick up `interaction effects' such as the XOR problem, interactions
could also be added to the list of partitioning variables.

\textbf{Regressors vs.\ partitioning variables:}
If regression models are partitioned, the question arises whether a certain covariate
should be included in $Y$ as a regressor or in $Z$ as a partitioning variable.
For categorical variables, this amounts to knowing/assuming the interactions or
trying to find them adaptively---for numerical variables, it amounts to knowing/assuming
a segment-wise linear relationship vs.\ approximating a possibly non-linear influence
by a step function. The separation can usually be made based on subject knowledge
as in the economic journals example. Other scenarios of this kind are conceivable:
e.g., in biostatistics it would be natural to fit a dose-response relationship and partition
it with respect to further experiment-specific covariables, or in business applications
a market segmentation could be carried out based on a standard demand function.
Finally, the variables entering the explanatory part of $Y$ and $Z$ can also be
overlapping---in this case, a trend-resistant fluctuation test is probably more
appropriate for partitioning.

\textbf{Large datasets:}
In Section~\ref{sec:illustration}, we have applied the suggested MOB algorithm
to several small to medium-sized data analysis tasks, showing that both satisfactory
predictive performance and interpretability can be achieved simultaneously.
The same is not necessarily true in very large data sets (both with many observations
and many partitioning variables), where there is usually a stronger trade-off
between complexity and predictive power. If there is a true underlying recursive
partition, the algorithm will recover this and stop growing the tree (as irrelevant
splits are only selected with probability $\alpha$). However, in practice, there is
typically no simple partition-based model (both with a simple partition and simple
local models) for large data sets. Thus, the algorithm can be used for approximating
a global model by recursive partitioning and fitting local models in the resulting
segments, typically by finding a more complex partition.
Alternatively, the practitioner can often reduce
the complexity of the partition by making the model $\mathcal{M}(Y, \theta)$ more
complex. In contrast, if the primary focus is model-based data exploration (rather than
prediction), a smaller tree can be grown, e.g., by decreasing $\alpha$ or by limiting
the maximal depth of the tree.


\section{Summary} \label{sec:summary}

A powerful, flexible and unified framework for model-based recursive
partitioning is suggested. It builds on parametric models which are
well-established in the statistical theory and whose parameters can be
easily interpreted by subject-matter scientists. Thus, it can not only
model the mean but also other properties of a parameterized
distribution. Furthermore, it can be
employed to partition regression relationships, such as GLMs or survival regression.
It aims at minimizing a clearly defined objective function (and not certain
heuristics) by a greedy forward search and is unbiased due to separation of
variable and split point selection.

Within the genuine statistical framework proposed in this paper, practitioners
can assess whether one (standard) global parametric model fits their data or whether
it is more appropriate to partition it with respect to further covariates.
If so, the partitioning variables and their split points are selected separately
in a forward search that controls the type I error rates for the variable selection
in each node. This formulation of the algorithm ensures that interpretations 
obtained from graphical representations of the corresponding tree-structured models
are valid in a statistical sense.

%% \section{Extensions, Proofs, ToDo}
%% 
%% \begin{itemize}
%%
%%   %% We just claim that MOB also works for survival regression. If a 
%%   %% referee wants more details, we can still think about it.
%%   \item FCLT: Cox-PH probably needs a different proof for the FCLT than in
%%     \cite{ZeileisHornik2007}. The score function $\psi$ depends not only on
%%     observation $i$ but is conditioned on all observation with a greater
%%     survival time (...I think).
%%
%%   %% Before making one of the referees want something like that, we don't
%%   %% mention it in the current version.
%%   \item Proof: it can probably be easily shown that binary splitting leads
%%     to partial parameter consistency in the sense of \cite{Chong1995}.
%%     This could justify a binary split strategy and multi-way splits just have
%%     to be outlined.
%%
%%   %% If there is just one variable with an influence, this follows trivially
%%   %% from the power properties...if there are several variables with breaks
%%   %% there could be non-trivial interactions. Better not mention it currently.
%%   \item Proof: Do partial consistency proofs exist for the variable selection
%%     problem? The approach taken in the ``unbiased'' frameworks is usually to say
%%     that variables without any association are selected just with probability $\alpha$.
%%     But when there are several variables \textbf{with} association/shifts, does
%%     this procedure select the variable with the strongest association/shift?
%% \end{itemize}

%% <TH> address missing in Stock & Watson reference </TH>

\section{Computational details}

All results were obtained using
\proglang{R}~\Sexpr{paste(R.Version()[6:7], collapse = ".")}---with the packages
\pkg{party}~\Sexpr{get_version("party")}, \pkg{strucchange}~\Sexpr{get_version("strucchange")},
\pkg{modeltools}~\Sexpr{get_version("modeltools")}, \pkg{survival}~\Sexpr{get_version("survival")},
and \pkg{vcd}~\Sexpr{get_version("vcd")}.
A detailed vignette reproducing the empirical analysis for the Boston housing and Pima Indians
diabetes data sets is available as \code{vignette("MOB", package = "party")}.

\bibliography{mob}

\end{document}

\begin{appendix}

\section[R code]{\proglang{R} code}

The following \proglang{R} code is sufficient for reproducing the empirical
examples from Section~\ref{sec:illustration} including data pre-processing, 
model fitting and visualization.

\begin{verbatim}
library("party")

data("Journals", package = "Ecdat")
journals <- Journals[, c("libprice", "society", "citestot")]
journals$oclc <- log(Journals$oclc)
journals$citeprice <- log(Journals$libprice/Journals$citestot)
journals$age <- 2000 - Journals$date1
journals$chars <- Journals$charpp*Journals$pages/10^6
mobJ <- mob(oclc ~ citeprice | society + citestot + age + chars + libprice,
  data = journals, model = linearModel, control = mob_control(minsplit = 10))
plot(mobJ)

data("BostonHousing", package = "mlbench")
BostonHousing$lstat <- log(BostonHousing$lstat)
BostonHousing$rm <- BostonHousing$rm^2
BostonHousing$chas <- factor(BostonHousing$chas)
BostonHousing$rad <- factor(BostonHousing$rad, ordered = TRUE)
mobBH <- mob(medv ~ lstat + rm | zn + indus + chas + nox + age + dis + rad +
  tax + crim + b + ptratio, data = BostonHousing,
  control = mob_control(minsplit = 40), model = linearModel)
plot(mobBH)

data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes <- na.omit(PimaIndiansDiabetes2[,-c(4, 5)])
mobPID <- mob(diabetes ~ glucose | pregnant + pressure + mass + pedigree + age,
  data = PimaIndiansDiabetes, model = glinearModel,
  control = mob_control(minsplit = 40), family = binomial())
plot(mobPID)

data("GBSG2", package = "ipred")
nloglik <- function(x) -logLik(x)
GBSG2$time <- GBSG2$time/365
mobGBSG2 <- mob(Surv(time, cens) ~ horTh + pnodes | progrec + menostat +
  estrec + menostat + age + tsize + tgrade, data = GBSG2, model = survReg,
  control = mob_control(objfun = nloglik, minsplit = 40))
plot(mobGBSG2, terminal = node_scatterplot, tp_args = list(yscale = c(-0.1, 11)))
\end{verbatim}

\end{appendix}
