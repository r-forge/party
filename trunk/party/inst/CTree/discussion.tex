
\begin{center}
\section{DISCUSSION}
\end{center}

In this paper, recursive binary partitioning with piecewise constant fits, 
a popular tool for regression analysis, is
embedded into a well-defined framework of conditional inference
procedures. Both the overfitting and variable selection problems induced by a
recursive fitting procedure are solved by the application of the 
appropriate statistical test procedures to both variable selection and
stopping. Therefore, the conditional inference trees suggested
in this paper are not just heuristics but non-parametric models with 
well-defined theoretical background. The methodology is generally applicable to
regression problems with arbitrary measurement scales of responses and
covariates.
%%To our knowledge, there exist only two approaches for recursive binary partitioning 
%%for ordinal regression. \cite{classifica:1984} introduce the `ordered twoing' 
%%criterion (p.~108) and some implementations of `CHAID' are able to deal with 
%%ordinal response variables. Here, the conditional distribution of linear
%%statistics appropriate for ordinal variables, for example the statistic 
%%of the linear-by-linear association test, is the basis of a non-parametric
%%approach extending the toolbox of ordinal regression models described by
%%\cite{KauermannTutz2003}.
In addition to its advantageous statistical properties, 
our framework is computationally attractive since we do not need to
evaluate all $2^{K-1} - 1$ possible splits of a nominal covariate at $K$
levels for the variable selection.
%% -> Section 3 "Computational Complexity"
%%The computational complexity of the algorithm is of order $n$ and, 
%%for nominal covariates measured at $K$ levels, the evaluation of all
%%$2^{K-1} - 1$ possible splits is not necessary for the variable selection.
In contrast to algorithms incorporating pruning based on 
resampling, the models suggested here can be fitted deterministically, 
provided that the exact conditional distribution is not approximated by
Monte-Carlo methods. 
%% -> Section 3, "Splitting Criteria"
%%Although we restricted ourselves to binary splits, the incorporation of
%%multiway splits in step 2 of the algorithm is possible, for example 
%%utilizing the work of \cite{OBrien2004}.


The simulation and benchmarking experiments in Section~\ref{ec} support two
conclusions: Conditional inference trees as suggested in this paper select
variables in an unbiased way and the partitions induced by this recursive
partitioning algorithm are not affected by overfitting. Even in a very
simple simulation model, the partitions obtained from conditional inference
trees are, on average, closer to the true data partition compared to
partitions obtained from an exhaustive search procedure with pruning. 
When the response is independent of all covariates, the
proportion of incorrect decisions in the root node 
is limited by $\alpha$ and when the response is
associated with one of the covariates, conditional inference trees select
the correct covariate more often than the exhaustive search procedure.
In the light of these findings, the conditional inference trees seem to be
more appropriate for diagnostic purposes than exhaustive search procedures. 
The results of
the benchmarking experiments with real data show that the prediction
accuracy of conditional inference trees is competitive with the
prediction accuracy of both an exhaustive search procedure 
(\texttt{rpart}) and unbiased recursive partitioning (\texttt{QUEST}/\texttt{GUIDE})
which select the tree size by pruning. 
Therefore, our findings
contradict the common opinion that pruning procedures outperform algorithms
with internal stopping with respect to prediction accuracy. 
%%While we are
%%confident that 
%%the difference between the performance of conditional inference 
%%trees and \texttt{rpart} 
%%is due to the variable selection bias of the latter one, the superiority
%%of conditional inference trees compared to the other unbiased procedures cannot
%%be explained this way. 
From our point of view, internal stopping criteria based on hypothesis tests 
evaluated earlier \citep[see for example the results of][]{FrankWitten1998} suffer from 
that fact that the data are transformed in order to fit the requirements of
a certain test procedure, such as categorizing continuous variables for a
$\chi^2$ test,
%%(for example CHAID or GUIDE)
instead of choosing 
a test procedure defined for the original measurement scale of the
covariates.
%%(conditional inference trees).
%%Conditional inference trees are free of any assumptions regarding the distributional 
%%properties of the responses or covariates. 
%% In contrast, `FACT' and most of its successors rely on
%% test procedures bound to classical normal theory.

When the parameter $\alpha$ is interpreted as a pre-defined nominal level of
the permutation tests performed in every node of the tree, the tree
structures visualized in a way
similar to Figures~\ref{glaucoma}--\ref{mammoexp} are valid in a sense that
covariates without association to the response appear in a node only with a
probability not exceeding $\alpha$.  
Moreover, subject matter scientists are most likely more familiar with
the interpretation of $\alpha$ as pre-defined nominal level
of hypothesis tests rather than as a fine-tuned hyper parameter.
Although it is possible to choose
$\alpha$ in a data-dependent way when prediction accuracy is the main focus, 
the empirical experiments in Section
\ref{ec} show that the classical convention of $\alpha = 0.05$ performs
well compared to tree models optimizing the prediction accuracy directly. 
However, while the predictions obtained from
conditional inference trees are as good as the predictions of pruned
exhaustive search trees,
the partitions induced by both algorithms differ structurally. 
Therefore,
the interpretations obtained from conditional inference trees and trees
fitted by an exhaustive search without bias correction cannot be 
assumed to be equivalent. Thus, two rather different partitions, and
therefore models, may have equal prediction accuracy.
Since a key reason for the popularity of 
tree based methods
stems from their ability to represent the estimated regression relationship
in an intuitive way, interpretations drawn 
from regression trees must be taken with a grain of salt.

In summary, this paper introduces a statistical approach to recursive
partitioning. Formal hypothesis tests for both variable
selection and stopping criterion are established. This choice leads to 
tree structured regression models for all kinds of regression problems,
including models for censored, ordinal or multivariate response variables.
Because well-known concepts are the basis of variable selection and 
stopping criterion, the resulting models are easier to communicate to practitioners. 
Simulation and benchmark experiments indicate that conditional inference
trees are well-suited for both explanation and prediction.

%%Nevertheless, one should keep in mind that regression models
%%based on univariate rectangular splits can only serve as a rough
%%approximation to reality. Tree algorithms able to fit non-constant models in
%%terminal nodes (e.g. GUIDE) 
%%provide more flexibility. However, if prediction accuracy is
%%the main target the data analyst aims at, 
%%recursive partitioning procedures are
%%ruled out by much more accurate predictors like some form of ensemble
%%learning or support vector machines anyway.
