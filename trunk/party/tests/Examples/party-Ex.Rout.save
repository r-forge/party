
R version 2.10.1 (2009-12-14)
Copyright (C) 2009 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "party"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('party')
Loading required package: survival
Loading required package: splines
Loading required package: grid
Loading required package: modeltools
Loading required package: stats4
Loading required package: coin
Loading required package: mvtnorm
Loading required package: zoo
Loading required package: sandwich
Loading required package: strucchange
Loading required package: vcd
Loading required package: MASS
Loading required package: colorspace
> 
> assign(".oldSearch", search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("BinaryTree-class")
> ### * BinaryTree-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: BinaryTree Class
> ### Title: Class "BinaryTree"
> ### Aliases: BinaryTree-class weights weights-methods
> ###   weights,BinaryTree-method show,BinaryTree-method where where-methods
> ###   where,BinaryTree-method response response-methods
> ###   response,BinaryTree-method nodes nodes-methods
> ###   nodes,BinaryTree,integer-method nodes,BinaryTree,numeric-method
> ###   treeresponse treeresponse-methods treeresponse,BinaryTree-method
> ### Keywords: classes
> 
> ### ** Examples
> 
> 
>   set.seed(290875)
> 
>   airq <- subset(airquality, !is.na(Ozone))
>   airct <- ctree(Ozone ~ ., data = airq,   
+                  controls = ctree_control(maxsurrogate = 3))
> 
>   ### distribution of responses in the terminal nodes
>   plot(airq$Ozone ~ as.factor(where(airct)))
> 
>   ### get all terminal nodes from the tree
>   nodes(airct, unique(where(airct)))
[[1]]
5)*  weights = 48 

[[2]]
3)*  weights = 10 

[[3]]
6)*  weights = 21 

[[4]]
9)*  weights = 7 

[[5]]
8)*  weights = 30 

> 
>   ### extract weights and compute predictions
>   pmean <- sapply(weights(airct), function(w) weighted.mean(airq$Ozone, w))
> 
>   ### the same as
>   drop(Predict(airct))
  [1] 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917
  [9] 55.60000 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917
 [17] 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917 31.14286
 [25] 55.60000 18.47917 31.14286 48.71429 48.71429 31.14286 18.47917 18.47917
 [33] 18.47917 18.47917 18.47917 81.63333 81.63333 31.14286 81.63333 48.71429
 [41] 81.63333 81.63333 81.63333 81.63333 18.47917 31.14286 31.14286 55.60000
 [49] 31.14286 81.63333 81.63333 48.71429 55.60000 81.63333 81.63333 31.14286
 [57] 48.71429 81.63333 81.63333 81.63333 31.14286 55.60000 31.14286 31.14286
 [65] 81.63333 81.63333 81.63333 81.63333 81.63333 81.63333 48.71429 31.14286
 [73] 31.14286 18.47917 55.60000 18.47917 31.14286 31.14286 18.47917 18.47917
 [81] 31.14286 55.60000 81.63333 81.63333 81.63333 81.63333 81.63333 81.63333
 [89] 81.63333 81.63333 81.63333 81.63333 48.71429 31.14286 31.14286 18.47917
 [97] 18.47917 31.14286 18.47917 55.60000 18.47917 18.47917 55.60000 18.47917
[105] 18.47917 18.47917 31.14286 18.47917 18.47917 31.14286 18.47917 18.47917
[113] 55.60000 18.47917 18.47917 18.47917
> 
>   ### or
>   unlist(treeresponse(airct))
  [1] 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917
  [9] 55.60000 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917
 [17] 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917 18.47917 31.14286
 [25] 55.60000 18.47917 31.14286 48.71429 48.71429 31.14286 18.47917 18.47917
 [33] 18.47917 18.47917 18.47917 81.63333 81.63333 31.14286 81.63333 48.71429
 [41] 81.63333 81.63333 81.63333 81.63333 18.47917 31.14286 31.14286 55.60000
 [49] 31.14286 81.63333 81.63333 48.71429 55.60000 81.63333 81.63333 31.14286
 [57] 48.71429 81.63333 81.63333 81.63333 31.14286 55.60000 31.14286 31.14286
 [65] 81.63333 81.63333 81.63333 81.63333 81.63333 81.63333 48.71429 31.14286
 [73] 31.14286 18.47917 55.60000 18.47917 31.14286 31.14286 18.47917 18.47917
 [81] 31.14286 55.60000 81.63333 81.63333 81.63333 81.63333 81.63333 81.63333
 [89] 81.63333 81.63333 81.63333 81.63333 48.71429 31.14286 31.14286 18.47917
 [97] 18.47917 31.14286 18.47917 55.60000 18.47917 18.47917 55.60000 18.47917
[105] 18.47917 18.47917 31.14286 18.47917 18.47917 31.14286 18.47917 18.47917
[113] 55.60000 18.47917 18.47917 18.47917
> 
>   ### don't use the mean but the median as prediction in each terminal node
>   pmedian <- sapply(weights(airct), function(w) 
+                  median(airq$Ozone[rep(1:nrow(airq), w)]))
> 
>   plot(airq$Ozone, pmean, col = "red")
>   points(airq$Ozone, pmedian, col = "blue")
> 
> 
> 
> cleanEx()
> nameEx("RandomForest-class")
> ### * RandomForest-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: RandomForest-class
> ### Title: Class "RandomForest"
> ### Aliases: RandomForest-class treeresponse,RandomForest-method
> ###   weights,RandomForest-method show,RandomForest-method
> ### Keywords: classes
> 
> ### ** Examples
> 
> 
>     set.seed(290875)
> 
>     ### honest (i.e., out-of-bag) cross-classification of 
>     ### true vs. predicted classes
>     table(mammoexp$ME, predict(cforest(ME ~ ., data = mammoexp, 
+                                control = cforest_classical(ntree = 50)), 
+                                OOB = TRUE))
               
                Never Within a Year Over a Year
  Never           207            27           0
  Within a Year    64            40           0
  Over a Year      62            12           0
> 
> 
> 
> cleanEx()
> nameEx("Transformations")
> ### * Transformations
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Transformations
> ### Title: Function for Data Transformations
> ### Aliases: ptrafo ff_trafo
> ### Keywords: manip
> 
> ### ** Examples
> 
> 
>   ### rank a variable
>   ptrafo(data.frame(y = 1:20), 
+          numeric_trafo = function(x) rank(x, na.last = "keep"))
        
 [1,]  1
 [2,]  2
 [3,]  3
 [4,]  4
 [5,]  5
 [6,]  6
 [7,]  7
 [8,]  8
 [9,]  9
[10,] 10
[11,] 11
[12,] 12
[13,] 13
[14,] 14
[15,] 15
[16,] 16
[17,] 17
[18,] 18
[19,] 19
[20,] 20
attr(,"assign")
[1] 1
> 
>   ### dummy coding of a factor
>   ptrafo(data.frame(y = gl(3, 9)))
   1 2 3
1  1 0 0
2  1 0 0
3  1 0 0
4  1 0 0
5  1 0 0
6  1 0 0
7  1 0 0
8  1 0 0
9  1 0 0
10 0 1 0
11 0 1 0
12 0 1 0
13 0 1 0
14 0 1 0
15 0 1 0
16 0 1 0
17 0 1 0
18 0 1 0
19 0 0 1
20 0 0 1
21 0 0 1
22 0 0 1
23 0 0 1
24 0 0 1
25 0 0 1
26 0 0 1
27 0 0 1
attr(,"assign")
[1] 1 1 1
> 
> 
> 
> 
> cleanEx()
> nameEx("cforest")
> ### * cforest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cforest
> ### Title: Random Forest
> ### Aliases: cforest proximity
> ### Keywords: tree
> 
> ### ** Examples
> 
> 
>     set.seed(290875)
> 
>     ### honest (i.e., out-of-bag) cross-classification of
>     ### true vs. predicted classes
>     table(mammoexp$ME, predict(cforest(ME ~ ., data = mammoexp, 
+                                control = cforest_classical(ntree = 50)),
+                                OOB = TRUE))
               
                Never Within a Year Over a Year
  Never           207            27           0
  Within a Year    64            40           0
  Over a Year      62            12           0
> 
>     ### fit forest to censored response
>     if (require("ipred")) {
+ 
+         data("GBSG2", package = "ipred")
+         bst <- cforest(Surv(time, cens) ~ ., data = GBSG2, 
+                    control = cforest_classical(ntree = 50))
+ 
+         ### estimate conditional Kaplan-Meier curves
+         treeresponse(bst, newdata = GBSG2[1:2,], OOB = TRUE)
+ 
+         ### if you can't resist to look at individual trees ...
+         party:::prettytree(bst@ensemble[[1]], names(bst@data@get("input")))
+     }
Loading required package: ipred
Loading required package: rpart
Loading required package: mlbench
Loading required package: nnet
Loading required package: class
1) tgrade <= 1; criterion = 5.039, statistic = 5.039
  2) age <= 51; criterion = 2.187, statistic = 2.187
    3)*  weights = 0 
  2) age > 51
    4) horTh == {}; criterion = 1.351, statistic = 1.351
      5)*  weights = 0 
    4) horTh == {}
      6)*  weights = 0 
1) tgrade > 1
  7) pnodes <= 7; criterion = 5.371, statistic = 5.371
    8) progrec <= 89; criterion = 2.701, statistic = 2.701
      9) pnodes <= 3; criterion = 1.78, statistic = 1.78
        10) progrec <= 63; criterion = 1.812, statistic = 1.812
          11) menostat == {}; criterion = 2.344, statistic = 2.344
            12) pnodes <= 2; criterion = 1.385, statistic = 1.385
              13) horTh == {}; criterion = 2.757, statistic = 2.757
                14)*  weights = 0 
              13) horTh == {}
                15) progrec <= 5; criterion = 2.595, statistic = 2.595
                  16)*  weights = 0 
                15) progrec > 5
                  17)*  weights = 0 
            12) pnodes > 2
              18) tsize <= 23; criterion = 2.103, statistic = 2.103
                19)*  weights = 0 
              18) tsize > 23
                20)*  weights = 0 
          11) menostat == {}
            21) pnodes <= 1; criterion = 2.459, statistic = 2.459
              22)*  weights = 0 
            21) pnodes > 1
              23) tgrade <= 2; criterion = 2.534, statistic = 2.534
                24) pnodes <= 2; criterion = 1.356, statistic = 1.356
                  25)*  weights = 0 
                24) pnodes > 2
                  26)*  weights = 0 
              23) tgrade > 2
                27)*  weights = 0 
        10) progrec > 63
          28)*  weights = 0 
      9) pnodes > 3
        29) progrec <= 61; criterion = 2.435, statistic = 2.435
          30) estrec <= 77; criterion = 1.791, statistic = 1.791
            31) horTh == {}; criterion = 1.305, statistic = 1.305
              32) tgrade <= 2; criterion = 1.556, statistic = 1.556
                33)*  weights = 0 
              32) tgrade > 2
                34)*  weights = 0 
            31) horTh == {}
              35) estrec <= 4; criterion = 1.443, statistic = 1.443
                36)*  weights = 0 
              35) estrec > 4
                37) age <= 44; criterion = 1.863, statistic = 1.863
                  38)*  weights = 0 
                37) age > 44
                  39)*  weights = 0 
          30) estrec > 77
            40)*  weights = 0 
        29) progrec > 61
          41)*  weights = 0 
    8) progrec > 89
      42) horTh == {}; criterion = 3.499, statistic = 3.499
        43) estrec <= 117; criterion = 1.913, statistic = 1.913
          44) menostat == {}; criterion = 3.365, statistic = 3.365
            45) pnodes <= 3; criterion = 2.629, statistic = 2.629
              46) tsize <= 21; criterion = 1.522, statistic = 1.522
                47)*  weights = 0 
              46) tsize > 21
                48)*  weights = 0 
            45) pnodes > 3
              49)*  weights = 0 
          44) menostat == {}
            50)*  weights = 0 
        43) estrec > 117
          51)*  weights = 0 
      42) horTh == {}
        52) tgrade <= 2; criterion = 1.639, statistic = 1.639
          53)*  weights = 0 
        52) tgrade > 2
          54)*  weights = 0 
  7) pnodes > 7
    55) horTh == {}; criterion = 3.183, statistic = 3.183
      56)*  weights = 0 
    55) horTh == {}
      57) progrec <= 16; criterion = 2.87, statistic = 2.87
        58)*  weights = 0 
      57) progrec > 16
        59) tsize <= 29; criterion = 2.421, statistic = 2.421
          60)*  weights = 0 
        59) tsize > 29
          61)*  weights = 0 
> 
>     ### proximity, see ?randomForest
>     iris.cf <- cforest(Species ~ ., data = iris, 
+                        control = cforest_unbiased(mtry = 2))
>     iris.mds <- cmdscale(1 - proximity(iris.cf), eig = TRUE)
>     op <- par(pty="s")
>     pairs(cbind(iris[,1:4], iris.mds$points), cex = 0.6, gap = 0, 
+           col = c("red", "green", "blue")[as.numeric(iris$Species)],
+           main = "Iris Data: Predictors and MDS of Proximity Based on cforest")
>     par(op)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("ctree")
> ### * ctree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Conditional Inference Trees
> ### Title: Conditional Inference Trees
> ### Aliases: ctree conditionalTree
> ### Keywords: tree
> 
> ### ** Examples
> 
> 
>     set.seed(290875)
> 
>     ### regression
>     airq <- subset(airquality, !is.na(Ozone))
>     airct <- ctree(Ozone ~ ., data = airq, 
+                    controls = ctree_control(maxsurrogate = 3))
>     airct

	 Conditional inference tree with 5 terminal nodes

Response:  Ozone 
Inputs:  Solar.R, Wind, Temp, Month, Day 
Number of observations:  116 

1) Temp <= 82; criterion = 1, statistic = 56.086
  2) Wind <= 6.9; criterion = 0.998, statistic = 12.969
    3)*  weights = 10 
  2) Wind > 6.9
    4) Temp <= 77; criterion = 0.997, statistic = 11.599
      5)*  weights = 48 
    4) Temp > 77
      6)*  weights = 21 
1) Temp > 82
  7) Wind <= 10.3; criterion = 0.997, statistic = 11.712
    8)*  weights = 30 
  7) Wind > 10.3
    9)*  weights = 7 
>     plot(airct)
>     mean((airq$Ozone - predict(airct))^2)
[1] 403.6668
> 
>     ### classification
>     irisct <- ctree(Species ~ .,data = iris)
>     irisct

	 Conditional inference tree with 4 terminal nodes

Response:  Species 
Inputs:  Sepal.Length, Sepal.Width, Petal.Length, Petal.Width 
Number of observations:  150 

1) Petal.Length <= 1.9; criterion = 1, statistic = 140.264
  2)*  weights = 50 
1) Petal.Length > 1.9
  3) Petal.Width <= 1.7; criterion = 1, statistic = 67.894
    4) Petal.Length <= 4.8; criterion = 0.999, statistic = 13.865
      5)*  weights = 46 
    4) Petal.Length > 4.8
      6)*  weights = 8 
  3) Petal.Width > 1.7
    7)*  weights = 46 
>     plot(irisct)
>     table(predict(irisct), iris$Species)
            
             setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         49         5
  virginica       0          1        45
> 
>     ### estimated class probabilities, a list
>     tr <- treeresponse(irisct, newdata = iris[1:10,])
> 
>     ### ordinal regression
>     mammoct <- ctree(ME ~ ., data = mammoexp) 
>     plot(mammoct)
> 
>     ### estimated class probabilities
>     treeresponse(mammoct, newdata = mammoexp[1:10,])
[[1]]
[1] 0.3990385 0.3798077 0.2211538

[[2]]
[1] 0.84070796 0.05309735 0.10619469

[[3]]
[1] 0.3990385 0.3798077 0.2211538

[[4]]
[1] 0.6153846 0.2087912 0.1758242

[[5]]
[1] 0.3990385 0.3798077 0.2211538

[[6]]
[1] 0.3990385 0.3798077 0.2211538

[[7]]
[1] 0.3990385 0.3798077 0.2211538

[[8]]
[1] 0.3990385 0.3798077 0.2211538

[[9]]
[1] 0.84070796 0.05309735 0.10619469

[[10]]
[1] 0.3990385 0.3798077 0.2211538

> 
>     ### survival analysis
>     if (require("ipred")) {
+         data("GBSG2", package = "ipred")
+         GBSG2ct <- ctree(Surv(time, cens) ~ .,data = GBSG2)
+         plot(GBSG2ct)
+         treeresponse(GBSG2ct, newdata = GBSG2[1:2,])        
+     }
Loading required package: ipred
Loading required package: rpart
Loading required package: mlbench
Loading required package: nnet
Loading required package: class
[[1]]
records   n.max n.start  events  median 0.95LCL 0.95UCL 
    686     248     248      88    2093    1814      NA 

[[2]]
records   n.max n.start  events  median 0.95LCL 0.95UCL 
    686     166     166      77    1701    1174    2018 

> 
>     ### if you are interested in the internals:
>     ## Not run: 
> ##D         browseURL(system.file("documentation/html/index.html", 
> ##D                               package = "party"))
> ##D     
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ctree_memory")
> ### * ctree_memory
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Memory Allocation
> ### Title: Memory Allocation
> ### Aliases: ctree_memory
> ### Keywords: misc
> 
> ### ** Examples
> 
> 
>     set.seed(290875)
> 
>     ### setup learning sample
>     airq <- subset(airquality, !is.na(Ozone))
>     ls <- dpp(conditionalTree, Ozone ~ ., data = airq)
> 
>     ### setup memory and controls 
>     mem <- ctree_memory(ls)
>     ct <- ctree_control(teststat = "max")
> 
>     ### fit 50 trees on bootstrap samples
>     bs <- rmultinom(50, nrow(airq), rep(1, nrow(airq))/nrow(airq))
>     storage.mode(bs) <- "double"
>     cfit <- conditionalTree@fit
>     system.time(ens <- apply(bs, 2, function(w) cfit(ls, ct, weights = w, 
+                                                      fitmem = mem)))
   user  system elapsed 
  0.248   0.000   0.251 
> 
> 
> 
> 
> cleanEx()
> nameEx("mammoexp")
> ### * mammoexp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mammoexp
> ### Title: Mammography Experience Study
> ### Aliases: mammoexp
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
>   ### fit tree with attached scores (equal to the default values)
>   mtree <- ctree(ME ~ .,data = mammoexp, 
+         scores = list(ME = 1:3, SYMPT = 1:4, DECT = 1:3))
>   mtree

	 Conditional inference tree with 3 terminal nodes

Response:  ME 
Inputs:  SYMPT, PB, HIST, BSE, DECT 
Number of observations:  412 

1) SYMPT <= Agree; criterion = 1, statistic = 29.933
  2)*  weights = 113 
1) SYMPT > Agree
  3) PB <= 8; criterion = 0.988, statistic = 9.17
    4)*  weights = 208 
  3) PB > 8
    5)*  weights = 91 
>   plot(mtree)
> 
> 
> 
> cleanEx()
> nameEx("mob")
> ### * mob
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mob
> ### Title: Model-based Recursive Partitioning
> ### Aliases: mob mob-class coef.mob deviance.mob fitted.mob logLik.mob
> ###   predict.mob print.mob residuals.mob sctest.mob summary.mob
> ###   weights.mob
> ### Keywords: tree
> 
> ### ** Examples
> 
> 
> set.seed(290875)
> 
> if(require("mlbench")) {
+ 
+ ## recursive partitioning of a linear regression model
+ ## load data
+ data("BostonHousing", package = "mlbench")
+ ## and transform variables appropriately (for a linear regression)
+ BostonHousing$lstat <- log(BostonHousing$lstat)
+ BostonHousing$rm <- BostonHousing$rm^2
+ ## as well as partitioning variables (for fluctuation testing)
+ BostonHousing$chas <- factor(BostonHousing$chas, levels = 0:1, 
+                              labels = c("no", "yes"))
+ BostonHousing$rad <- factor(BostonHousing$rad, ordered = TRUE)
+ 
+ ## partition the linear regression model medv ~ lstat + rm
+ ## with respect to all remaining variables:
+ fmBH <- mob(medv ~ lstat + rm | zn + indus + chas + nox + age + 
+                                 dis + rad + tax + crim + b + ptratio,
+   control = mob_control(minsplit = 40), data = BostonHousing, 
+   model = linearModel)
+ 
+ ## print the resulting tree
+ fmBH
+ ## or better visualize it
+ plot(fmBH)
+ 
+ ## extract coefficients in all terminal nodes
+ coef(fmBH)
+ ## look at full summary, e.g., for node 7
+ summary(fmBH, node = 7)
+ ## results of parameter stability tests for that node
+ sctest(fmBH, node = 7)
+ ## -> no further significant instabilities (at 5% level)
+ 
+ ## compute mean squared error (on training data)
+ mean((BostonHousing$medv - fitted(fmBH))^2)
+ mean(residuals(fmBH)^2)
+ deviance(fmBH)/sum(weights(fmBH))
+ 
+ ## evaluate logLik and AIC
+ logLik(fmBH)
+ AIC(fmBH)
+ ## (Note that this penalizes estimation of error variances, which
+ ## were treated as nuisance parameters in the fitting process.)
+ 
+ 
+ ## recursive partitioning of a logistic regression model
+ ## load data
+ data("PimaIndiansDiabetes", package = "mlbench")
+ ## partition logistic regression diabetes ~ glucose 
+ ## wth respect to all remaining variables
+ fmPID <- mob(diabetes ~ glucose | pregnant + pressure + triceps + 
+                                   insulin + mass + pedigree + age,
+   data = PimaIndiansDiabetes, model = glinearModel, 
+   family = binomial())
+ 
+ ## fitted model
+ coef(fmPID)
+ plot(fmPID)
+ plot(fmPID, tp_args = list(cdplot = TRUE))
+ }
Loading required package: mlbench
Warning: no admissable split found
> 
> 
> 
> cleanEx()
> nameEx("panelfunctions")
> ### * panelfunctions
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Panel Generating Functions
> ### Title: Panel-Generators for Visualization of Party Trees
> ### Aliases: node_inner node_terminal edge_simple node_surv node_barplot
> ###   node_boxplot node_hist node_density node_scatterplot node_bivplot
> ### Keywords: hplot
> 
> ### ** Examples
> 
> 
>   set.seed(290875)
> 
>   airq <- subset(airquality, !is.na(Ozone))
>   airct <- ctree(Ozone ~ ., data = airq)
> 
>   ## default: boxplots
>   plot(airct)
>   
>   ## change colors
>   plot(airct, tp_args = list(col = "blue", fill = hsv(2/3, 0.5, 1)))
>   ## equivalent to
>   plot(airct, terminal_panel = node_boxplot(airct, col = "blue", 
+                                             fill = hsv(2/3, 0.5, 1)))
> 
>   ### very simple; the mean is given in each terminal node
>   plot(airct, type = "simple")
> 
>   ### density estimates
>   plot(airct, terminal_panel = node_density)
>     
>   ### histograms 
>   plot(airct, terminal_panel = node_hist(airct, ymax = 0.06, 
+                                          xscale = c(0, 250)))
> 
> 
> 
> cleanEx()
> nameEx("plot.BinaryTree")
> ### * plot.BinaryTree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Plot BinaryTree
> ### Title: Visualization of Binary Regression Trees
> ### Aliases: plot.BinaryTree
> ### Keywords: hplot
> 
> ### ** Examples
> 
> 
>   set.seed(290875)
> 
>   airq <- subset(airquality, !is.na(Ozone))
>   airct <- ctree(Ozone ~ ., data = airq)
> 
>   ### regression: boxplots in each node
>   plot(airct, terminal_panel = node_boxplot, drop_terminal = TRUE)
> 
>   if(require("ipred")) {
+   ## classification: barplots in each node
+   data("GlaucomaM", package = "ipred")
+   glauct <- ctree(Class ~ ., data = GlaucomaM)
+   plot(glauct)
+   plot(glauct, inner_panel = node_barplot,
+     edge_panel = function(ctreeobj, ...) { function(...) invisible() },
+     tnex = 1)
+ 
+   ## survival: Kaplan-Meier curves in each node
+   data("GBSG2", package = "ipred")
+   gbsg2ct <- ctree(Surv(time, cens) ~ ., data = GBSG2)
+   plot(gbsg2ct)
+   plot(gbsg2ct, type = "simple")  
+   }
Loading required package: ipred
Loading required package: rpart
Loading required package: mlbench
Loading required package: nnet
Loading required package: class
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.mob")
> ### * plot.mob
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.mob
> ### Title: Visualization of MOB Trees
> ### Aliases: plot.mob
> ### Keywords: hplot
> 
> ### ** Examples
> 
> 
> set.seed(290875)
> 
> if(require("mlbench")) {
+ 
+ ## recursive partitioning of a linear regression model
+ ## load data
+ data("BostonHousing", package = "mlbench")
+ ## and transform variables appropriately (for a linear regression)
+ BostonHousing$lstat <- log(BostonHousing$lstat)
+ BostonHousing$rm <- BostonHousing$rm^2
+ ## as well as partitioning variables (for fluctuation testing)
+ BostonHousing$chas <- factor(BostonHousing$chas, levels = 0:1, 
+                              labels = c("no", "yes"))
+ BostonHousing$rad <- factor(BostonHousing$rad, ordered = TRUE)
+ 
+ ## partition the linear regression model medv ~ lstat + rm
+ ## with respect to all remaining variables:
+ fm <- mob(medv ~ lstat + rm | zn + indus + chas + nox + age + dis + 
+                               rad + tax + crim + b + ptratio,
+   control = mob_control(minsplit = 40), data = BostonHousing, 
+   model = linearModel)
+ 
+ ## visualize medv ~ lstat and medv ~ rm
+ plot(fm)
+ 
+ ## visualize only one of the two regressors
+ plot(fm, tp_args = list(which = "lstat"), tnex = 2)
+ plot(fm, tp_args = list(which = 2), tnex = 2)
+ 
+ ## omit fitted mean lines
+ plot(fm, tp_args = list(fitmean = FALSE))
+ 
+ ## mixed numerical and categorical regressors 
+ fm2 <- mob(medv ~ lstat + rm + chas | zn + indus + nox + age + 
+                                       dis + rad,
+   control = mob_control(minsplit = 100), data = BostonHousing, 
+   model = linearModel)
+ plot(fm2)
+ 
+ ## recursive partitioning of a logistic regression model
+ data("PimaIndiansDiabetes", package = "mlbench")
+ fmPID <- mob(diabetes ~ glucose | pregnant + pressure + triceps + 
+                                   insulin + mass + pedigree + age,
+   data = PimaIndiansDiabetes, model = glinearModel, 
+   family = binomial())
+ ## default plot: spinograms with breaks from five point summary
+ plot(fmPID)
+ ## use the breaks from hist() instead
+ plot(fmPID, tp_args = list(fivenum = FALSE))
+ ## user-defined breaks
+ plot(fmPID, tp_args = list(breaks = 0:4 * 50))
+ ## CD plots instead of spinograms
+ plot(fmPID, tp_args = list(cdplot = TRUE))
+ ## different smoothing bandwidth
+ plot(fmPID, tp_args = list(cdplot = TRUE, bw = 15))
+ 
+ }
Loading required package: mlbench
Warning: no admissable split found
> 
> 
> 
> cleanEx()
> nameEx("readingSkills")
> ### * readingSkills
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: readingSkills
> ### Title: Reading Skills
> ### Aliases: readingSkills
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
>    set.seed(290875)
>    readingSkills.cf <- cforest(score ~ ., data = readingSkills,
+        control = cforest_unbiased(mtry = 2, ntree = 50))
> 
>    varimp(readingSkills.cf)
nativeSpeaker           age      shoeSize 
     13.13315      81.79755      15.69448 
> 
>    varimp(readingSkills.cf, conditional = TRUE)
nativeSpeaker           age      shoeSize 
    11.616286     51.106067      1.233911 
> 
> 
> 
> 
> cleanEx()
> nameEx("reweight")
> ### * reweight
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: reweight
> ### Title: Re-fitting Models with New Weights
> ### Aliases: reweight reweight.linearModel reweight.glinearModel
> ### Keywords: regression
> 
> ### ** Examples
> 
>   ## fit cars regression
>   mf <- dpp(linearModel, dist ~ speed, data = cars)
>   fm <- fit(linearModel, mf)
>   fm
Linear model with coefficients:
(Intercept)        speed  
    -17.579        3.932  
>   
>   ## re-fit, excluding the last 4 observations
>   ww <- c(rep(1, 46), rep(0, 4))
>   reweight(fm, ww)
Linear model with coefficients:
(Intercept)        speed  
     -8.723        3.210  
> 
> 
> 
> cleanEx()
> nameEx("varimp")
> ### * varimp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: varimp
> ### Title: Variable Importance
> ### Aliases: varimp
> ### Keywords: tree
> 
> ### ** Examples
> 
>     
>    set.seed(290875)
>    readingSkills.cf <- cforest(score ~ ., data = readingSkills, 
+        control = cforest_unbiased(mtry = 2, ntree = 50))
> 
>    # standard importance
>    varimp(readingSkills.cf)
nativeSpeaker           age      shoeSize 
     13.13315      81.79755      15.69448 
> 
>    # conditional importance, may take a while...
>    varimp(readingSkills.cf, conditional = TRUE)
nativeSpeaker           age      shoeSize 
    11.616286     51.106067      1.233911 
> 
> 
> 
> 
> ### * <FOOTER>
> ###
> cat("Time elapsed: ", proc.time() - get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  43.506 0.168 44.348 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
