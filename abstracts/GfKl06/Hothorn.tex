\documentclass{GfKl2006}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{times}

\begin{document}

\title*{Unbiased Recursive Partitioning I:\\
       A Non-parametric Conditional Inference Framework}

\author{
Torsten Hothorn\inst{1}
\and
Kurt Hornik\inst{2}
\and
Achim Zeileis\inst{2}
}


\institute{
Institut f\"ur Medizininformatik, Biometrie und Epidemiologie\\
     Friedrich-Alexander-Universit\"at Erlangen-N\"urnberg\\
     Waldstra{\ss}e 6, D-91054 Erlangen, Germany \\
     \texttt{Torsten.Hothorn@rzmail.uni-erlangen.de}
\and
Department f\"ur Statistik und Mathematik,
             Wirtschaftsuniversit\"at Wien \\
       Augasse 2-6, A-1090 Wien, Austria \\
       \texttt{Kurt.Hornik@wu-wien.ac.at} \\
       \texttt{Achim.Zeileis@wu-wien.ac.at}
}

\maketitle

\begin{abstract}
Recursive partitioning is a popular tool for
regression analysis. Two fundamental problems of exhaustive
search procedures usually applied to fit such models
have been known for a long time: overfitting and a selection bias towards
covariates with many possible splits or missing values. While pruning
procedures are able to solve the overfitting problem, the variable
selection
bias still seriously effects the interpretability of tree-structured
regression models. It has been known for some time now that the variable
selection
problem can be solved by a separation of variable selection and cutpoint
estimation. For some special cases such unbiased procedures have been
suggested,
however lacking a common theoretical foundation.

In this pair of presentations, we propose two unified frameworks for
recursive partitioning with unbiased variable selection. In the first part,
a non-parametric procedure based on conditional inference techniques is suggested.
The second part is concerned with partitioning based on parametric models utilizing 
parameter instability tests for variable selection.

Here, the recursive partitioning algorithm
utilizing non-parametric conditional inference procedures is discussed.
Stopping criteria based on multiple test procedures are implemented and it
is shown that the predictive performance of the resulting trees is as good as
the performance of established exhaustive search procedures. It turns out
that the partitions and therefore the models induced by both approaches
are structurally different, indicating the need for an unbiased variable
selection. The methodology presented here is applicable to all kinds of
regression problems, including nominal, ordinal, numeric, censored as well as
multivariate response variables and arbitrary measurement scales of the
covariates. Data from studies on animal abundance, glaucoma
classification, node positive breast cancer survival and mammography experience
are re-analyzed.
\end{abstract}

\noindent
\textbf{Key words:} permutation tests, variable selection, multiple testing,
                    ordinal regression trees, multivariate regression trees

\end{document}
